<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.0.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lt4hyl.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Sequence ModelsRNN的引入在使用标准的神经网络（如：DNN）输入序列化数据（如：单词的 one-hot编码），可能存在的问题：  输入和输出，对不同的实例会有不同长度，虽然可以用0填充到最大长度，但这个方法不好； 神经网络太简单，不会共享从不同文本位置学习到的特征。（输入前后的序列特征、上下文特征，没有学习到）。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习笔记（5）">
<meta property="og:url" content="http://lt4hyl.top/2020-09-02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89.html">
<meta property="og:site_name" content="Lancelot的小站">
<meta property="og:description" content="Sequence ModelsRNN的引入在使用标准的神经网络（如：DNN）输入序列化数据（如：单词的 one-hot编码），可能存在的问题：  输入和输出，对不同的实例会有不同长度，虽然可以用0填充到最大长度，但这个方法不好； 神经网络太简单，不会共享从不同文本位置学习到的特征。（输入前后的序列特征、上下文特征，没有学习到）。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200707111215551.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200707130537643.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200707130705606.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200707131549814.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200707160246000.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200707165720798.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/GRU1.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200707172249522.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200707173508075.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/LSTM1.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/LSTM2.jpg">
<meta property="og:image" content="http://lt4hyl.top/pictures/LSTM3.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200707174716353.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/BRNN2.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/BRNN1.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200707175302820.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200709095552184.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200709104821019.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200709113108113.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200709113343470.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200709114623503.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/word2vec.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/CBOW.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/Skip-gram.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/CBOW1.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/Skip-gram1.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200710103828084.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200710105028651.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200710104613615.png">
<meta property="article:published_time" content="2020-09-02T07:01:52.000Z">
<meta property="article:modified_time" content="2020-09-03T08:12:48.912Z">
<meta property="article:author" content="刘涛">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="基础知识">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://lt4hyl.top/pictures/image-20200707111215551.png">

<link rel="canonical" href="http://lt4hyl.top/2020-09-02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>深度学习笔记（5） | Lancelot的小站</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?43b02ed86a701dce319ff86325707452";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Lancelot的小站</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">记录点滴成长：AI所向，吾之所往</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://lt4hyl.top/2020-09-02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="刘涛">
      <meta itemprop="description" content="为人民日益增长的美好生活需要而读书：机器学习、深度学习、c++、python、C#">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lancelot的小站">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习笔记（5）
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-09-02 15:01:52" itemprop="dateCreated datePublished" datetime="2020-09-02T15:01:52+08:00">2020-09-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-09-03 16:12:48" itemprop="dateModified" datetime="2020-09-03T16:12:48+08:00">2020-09-03</time>
              </span>

          
            <span id="/2020-09-02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89.html" class="post-meta-item leancloud_visitors" data-flag-title="深度学习笔记（5）" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>7.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>7 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Sequence-Models"><a href="#Sequence-Models" class="headerlink" title="Sequence Models"></a>Sequence Models</h1><h2 id="RNN的引入"><a href="#RNN的引入" class="headerlink" title="RNN的引入"></a>RNN的引入</h2><p>在使用标准的神经网络（如：DNN）输入序列化数据（如：单词的 one-hot编码），可能存在的问题：</p>
<ul>
<li>输入和输出，对不同的实例会有不同长度，虽然可以用0填充到最大长度，但这个方法不好；</li>
<li>神经网络太简单，不会共享从不同文本位置学习到的特征。（输入前后的序列特征、上下文特征，没有学习到）。</li>
</ul>
<p><img src="/pictures/image-20200707111215551.png" alt="image-20200707111215551"></p>
<a id="more"></a>
<p>由此需要能学习到<strong>序列特征</strong>、<strong>上下文信息</strong>的神经网络，因此提出了标准的RNN（下文会介绍），但标准的RNN也存在自己的问题，如：只用序列中的早期数据做预测，即只使用输入或使用输入序列中之前信息，但不使用序列中后面的信息。以及只在一确定的时间内预测。因此对于RNN也有它的许多变种。</p>
<p>RNN的<strong>前向传播过程</strong>，如下图所示：</p>
<ul>
<li>RNN是使用同一个网络处理多个输入，在下图网络中每个输入都会产生对应的输出。</li>
</ul>
<p><img src="/pictures/image-20200707130537643.png" alt="image-20200707130537643"></p>
<p><img src="/pictures/image-20200707130705606.png" alt="image-20200707130705606"></p>
<p>RNN的<strong>反向传播过程</strong>，如下图所示：</p>
<ul>
<li>下图所示的网络：输入序列长度等于输出序列长度；</li>
<li>此时的反向传播过程随着输入序列的顺序逆向进行，每一个预测值与真实值的比较都会计入到梯度中，也都会对网络进行一次更新。输出序列后端的梯度会向前端传递。</li>
</ul>
<p><img src="/pictures/image-20200707131549814.png" alt="image-20200707131549814"></p>
<p>RNN的不同类型，如下图所示：</p>
<ul>
<li>many-to-many1：即Tx=Ty,输入长度等于输出长度的。</li>
<li>many-to-one： 如，Sentiment classification（情感分类），输入一段文本，输出单个Y值。</li>
<li>one-to-one：之前的标准神经网络（输入单个值，经过线性变换和激活函数后输出一个值）。</li>
<li>one-to-many：如，音乐产生器，用户输入偏好，自动产生一段音乐。</li>
<li>many-to-many2：Tx!=Ty，输入长度不等于输出长度的，如，机器翻译。</li>
</ul>
<p><img src="/pictures/image-20200707160246000.png" alt="image-20200707160246000"></p>
<h2 id="Language-Model-and-Sequence-generation"><a href="#Language-Model-and-Sequence-generation" class="headerlink" title="Language Model and Sequence generation"></a>Language Model and Sequence generation</h2><h2 id="语言模型（Language-Model-）"><a href="#语言模型（Language-Model-）" class="headerlink" title="语言模型（Language Model ）"></a>语言模型（Language Model ）</h2><p>语言模型的定义：即估计特定单词序列（一个句子，一串文本）的概率。</p>
<ul>
<li>通俗点说：即判断一个语言序列是否是正常语句，即是否是<strong>人话</strong>。</li>
<li>标准定义如下：</li>
</ul>
<script type="math/tex; mode=display">
对于语言序列 w_1,w_2,...,w_n，语言模型就是计算该序列的概率，即 P(w_1,w_2,...,w_n) \\\\
例如我们可以明显感知到：P(I \quad am \quad handsome)>P(handsome \quad i \quad am)</script><p>当尝试使用RNN建立一个简单的语言模型时，可以如下所示来直观理解：</p>
<ul>
<li>RNN的输入即为单词序列，RNN的输出即为这一串单词序列的概率。</li>
<li>在建模前，通常需要一个由大文集（Corpus）构成的训练集，如：大体量的或海量英语语句构成的英语文本。而后</li>
<li>对于训练集中的每一句话，首先要做的是<strong>标记化（Tokenize）</strong>，就是将各个单词映射到一个 one hot 向量上。<ul>
<li>具体来说对 Corpus 中出现过的单词进行统计形成一部词典或者直接使用现有的词典。假设词典有1万个单词，则对 Corpus 中的每个单词都可以转化为一个 1万维的 one hot 向量。</li>
<li>在句子结束处，通常加上一个EOS(End Of Sentence)标记</li>
<li>对未出现在单词表中的单词，打上UNK标记（全局唯一标记），表示未知单词</li>
<li>完成<strong>标记化</strong>的基本步骤：是将输入句子映射成各个标记，或单词表中单词的集合。</li>
</ul>
</li>
<li>用RNN为不同序列的概率建模：即在给定前<code>n</code>个单词序列的概率下，预测<code>n+1</code>个单词的概率，即可得到<code>n+1</code>个单词序列同时出现的概率。</li>
</ul>
<p>这里使用的是<strong>词级（Word-level）</strong>语言模型，也就是每个标记是针对单词，但也有<strong>字符级语言模型（Character-level）</strong>也就是每个标记都针对字符（如：’ab;!、#）。但通常意义上字符级语言模型不如词级语言模型好，如：在找出句首对句尾的，长距离的对应关系上。并且字符级语言模型在训练上也更为艰难。</p>
<p>在语言模型定义好了以后，就需要通过前向传播/后向传播来训练该模型，但RNN所面临的问题之一—<strong>梯度消失</strong>，会使得训练无法有效进行，产生原因如下：</p>
<ul>
<li>基本的RNN模型有附近效应（Local influences）：输出值<code>Yhat</code>，主要受接近于<code>Yhat</code>的值的影响；所以靠后的输入很难影响到靠前的输入。反向传播时就很难做到，将靠后输入的影响反馈到序列开始的地方，从而去修改神经网络。</li>
<li>由于句子中可能有<strong>长期依赖</strong>，而基本的RNN模型无法很好的捕获不同词之间的长期依赖性。</li>
</ul>
<p>因此也就有了GRU（Gated Recurrent Unit）/LSTM（Long Short-Term Memory）的提出，用于显著的减轻梯度消失问题。</p>
<h2 id="Gated-Recurrent-Unit-GRU"><a href="#Gated-Recurrent-Unit-GRU" class="headerlink" title="Gated Recurrent Unit(GRU)"></a>Gated Recurrent Unit(GRU)</h2><p>参考论文：</p>
<p><img src="/pictures/image-20200707165720798.png" alt="image-20200707165720798"></p>
<p>GRU的总体结构如下图所示：</p>
<p><img src="/pictures/GRU1.png" alt="GRU1"></p>
<script type="math/tex; mode=display">
其中 r_t 表示重置门（reset gate），z_t 表示更新门（update gate）。\\\\
首先计算更新门，其值在0到1之间：z_t = \sigma (W_z *[h_{t-1},x_t]) \\\\
其中h_{t-1}为上一个隐藏状态，x_t为当前的输入。\\\\
然后通过一个sigmoid函数映射到[0,1]之间；\\\\
该值决定了上一个隐藏状态有多少信息被保留下来\\\\
且新的内容有多少需要被被记忆,传递给下一状态。\\\\
接着是重置门，它的主要作用就是过去的多少信息需要被遗忘 \\\\ 
其计算公式为：r_t = \sigma(W_r*[h_{t-1},x_t]) ,它的值也会映射到[0,1]之间；\\\\
更新门与遗忘门类似，只不过参数不同 \\\\
重置门通过重置h_{t-1},也就是有多少信息需要被遗忘 \\\\
然后与当前输入x_t 一起送到tanh函数里得到新的记忆内容\widetilde{h}_t\\\\
其计算公式为：\widetilde{h}_t =tanh(W*[r_t*h_{t-1},x_t]) \\\\
最后是当前时间步的记忆计算公式为：h_t = z_t*\widetilde{h}_t+(1-z_t)*h_{t-1} \\\\
前述信息和当前信息就会被传递到下一个GRU单元里。</script><p>GRU的好处：</p>
<ul>
<li>当你从左向右扫描句子时候，可以决定，何时更新一个特定的内存单元, 然后保持不变，直到你真正需要它的时候。即使是在句子中更早些时候被设置的值，也可从记忆单元中取出。</li>
<li>采用<code>Sigmoid</code>激活函数，重置门很容易被设置为0，这样当前信息 <code>xt</code> 就很容易保持原值不变，这非常好的维持了记忆单元的值。</li>
<li>因此这可以很显著的降低梯度弥散（梯度消失）的问题，允许一个神经网络用友很长范围的依赖性。</li>
<li>相比于LSTM最大的优势在于：<strong>简单（因为只有两个门），计算开销小，更加适用于大规模数据集。</strong></li>
</ul>
<h2 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long Short-Term Memory(LSTM)"></a><strong>Long Short-Term Memory</strong>(LSTM)</h2><p>参考论文：</p>
<p><img src="/pictures/image-20200707172249522.png" alt="image-20200707172249522"></p>
<p>首先需要指出：LSTM只能极大的缓解RNN的梯度消失，但不能从根本上解决，所幸的是大多数的任务场景下实验表明，LSTM都能够取得很好的结果。</p>
<p>单个LSTM单元的结构如下图所示：</p>
<p><img src="/pictures/image-20200707173508075.png" alt="image-20200707173508075"></p>
<p>另一种类似的结构表述如下图所示，两者是一致的：</p>
<p><img src="/pictures/LSTM1.png" alt="LSTM1.png"></p>
<p> 跟GRU相比其实挺相似的，其中Γu是更新门（也叫输入门），Γf是遗忘门，Γo是输出门。其中最重要的可能就是<strong>遗忘门</strong>了。</p>
<p>这几个门的作用如下：</p>
<ul>
<li>更新门（输入门）：作用是为了更新cell state的时候，来决定哪些值需要被更新。</li>
<li>遗忘门：公式如上图所示，其中σ为<code>sigmoid</code>函数,因此Γf的值在0到1之间，也就是说遗忘门通过看上一个隐藏状态（<em>a</em>\&lt;<em>t</em>−1>）和当前的输入(<em>x</em>\&lt;<em>t</em>>）来得到一个值（Γf），然后用Γf去点乘<em>C</em>\&lt;<em>t</em>−1>（上一个memory cell）去决定上一个memory cell的信息是否保留，0表示丢弃，1表示保留。</li>
<li>输出门：决定<code>cell state</code>里哪些值应该被输出（即下一个<code>cell state</code>的值）。</li>
</ul>
<p>更详细的LSTM结构如下图所示:</p>
<p><img src="/pictures/LSTM2.jpg" alt="LSTM2.png"></p>
<p>LSTM的展开图如下所示:</p>
<p><img src="/pictures/LSTM3.png" alt="LSTM3.png"></p>
<h2 id="LSTM是如何减轻梯度消失问题的"><a href="#LSTM是如何减轻梯度消失问题的" class="headerlink" title="LSTM是如何减轻梯度消失问题的?"></a>LSTM是如何减轻梯度消失问题的?</h2><p>RNN产生<strong>梯度消失</strong>的根本原因在于<strong>连乘</strong>导致 ：</p>
<ul>
<li>当连续多次求导后的导数小于1时，累成的梯度将逐渐趋近于0，造成梯度消失问题；</li>
<li>当连续多次求导后的导数大于1时，累成的梯度将逐渐趋近于无穷，造成梯度爆炸问题。</li>
</ul>
<script type="math/tex; mode=display">
从求导过程看:c^{<t>} = \Gamma_f*c^{<t-1>}+\Gamma_u* \widetilde{c}^{<t-1>} \\\\
\frac{\partial c_t}{\partial c_{t-1}} = \Gamma_f + ... ,后面不写了重点是这里的\Gamma_f \\\\
\Gamma_f 是遗忘门的输出值,1表示完全保留,0表示完全丢弃 \\\\
当将 \Gamma_f 设置成1 或者接近于1的数时,上述偏导数大概率会接近于1,也就确保了梯度的传递</script><p>因此LSTM 靠着 这种单元结构来保留梯度，通过遗忘门的设置，可以缓解梯度消失问题。</p>
<p>这里说是缓解，是因为LSTM只是在 <code>ct 到 ct-1</code> 这一条路径上解决了梯度消失问题,而其它路径依然存在梯度消失问题。</p>
<p>而遗忘门解决了RNN中的长期依赖问题，不管网络多深，都可以记住之前的信息。</p>
<h2 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h2><p>基本的RNN结构不能获取某个预测输出之后的信息，只能根据前面的输入做判断。在当前位置为了即考虑<strong>之前的输入</strong>也考虑<strong>之后的输入</strong>信息，提出了双向循环网络，也就是<code>Bidirectional RNN</code>，如下图所示：</p>
<p><img src="/pictures/image-20200707174716353.png" alt="image-20200707174716353"></p>
<p>双向循环网络不仅仅有基本循环网络的forward components，增加了反向的backward components(如图绿色划线部分)，连接方式同forward components一样。不同之处在于反向传递的部分可能具有不同的激活函数，并且值传递的方向是反着的。</p>
<p>上述过程构成了一个有向无环图(Acyclic graph)注意forward components和backward components是互不连接的。一般来说，来从前往后计算，再从后往前计算，计算过程相互独立，互不干扰。</p>
<p>双向循环网络的实际计算过程如下图所示：</p>
<p><img src="/pictures/BRNN2.png" alt="BRNN2"></p>
<p>这样计算每个预测的输出值时，可表示为：</p>
<p><img src="/pictures/BRNN1.png" alt="BRNN1"></p>
<p>双向循环网络的优点：</p>
<ul>
<li>考虑到了所有位置的输入，并且能在句子的任意位置输出预测值。</li>
<li>对于有大量文本的NLP问题，如果有完整的句子，想对句子或者单词进行某种预测，使用LSTM单元的双向循环网络是最为常用的，而且也是首选的方法。</li>
</ul>
<p>双向循环网络的缺点：</p>
<ul>
<li>是在任意位置输出预测之前，你需要完整的数据序列(比如一个完整句子)。</li>
<li>如：在一个语音识别的应用场景下，BRNN需要整个语音即说话人全部说完才能进行处理，实时性可能受到影响。因此会有其它的模块来解决这个问题。</li>
</ul>
<h2 id="Deep-RNN"><a href="#Deep-RNN" class="headerlink" title="Deep RNN"></a>Deep RNN</h2><p>上述所提到的RNN网络，其中的RNN单元只有一层隐藏层，可以想象将多个RNN单元堆叠在一起，那就形成了深度循环神经网络(deep RNN)。深层循环神经网络如下图所示：</p>
<p><img src="/pictures/image-20200707175302820.png" alt="image-20200707175302820"></p>
<h2 id="Word-Representation（词汇表征）"><a href="#Word-Representation（词汇表征）" class="headerlink" title="Word Representation（词汇表征）"></a>Word Representation（词汇表征）</h2><p>上述说明使用RNN建立语言模型时对每个单词使用的是 one-hot 编码，也叫 <code>“one-hot representation”</code>，即根据该词在词典中的位置，以一串只有一个位置是1，其余位置都是0的向量来表示。</p>
<ul>
<li>这种表示把每个词都孤立起来了，对相关词的泛化能力不强。（如：苹果、橘子是高度相关的）</li>
<li>形式上表现于：任意两个 one-hot 向量的内积都是0。这样向量之间的距离都是一样的，这样的表示没什么价值。因此需要进行改进。</li>
</ul>
<p>那么为了得到每个单词比较意义的向量表示，我们需要抽取每个词语的特征，以这些特征来表示该向量，会是一个比较好的选择，这个过程叫做：<strong>word embedding</strong>。</p>
<p><strong>word embedding</strong>：将单词的表示从<code>one-hot representation</code> 变换成 <code>Featurized representation</code>的过程。也就是试图用某些固定维度的特征值来描述不同的词，以达到更好的表述，如：苹果、橘子、国王。使得苹果和橘子的表示能更相近些，苹果和国王的表示相差较大。</p>
<p>经过这样一描述，这里的word embedding 跟计算机视觉里的人脸识别有点想象：前者将单词转化成固定维度的向量，后者将人脸照片转化成固定维度的向量。但这两者还是有重大区别，不可混为一谈。</p>
<ul>
<li>与人脸识别的重要区别：人脸识别是要训练一个网络，输入任何一张图片（即便不认识，意味着图片可以无穷多）也能得到一个固定维度的向量编码（<code>fixed encoding</code>）；而 word embedding 是针对 <strong>当前任务的词汇表（词汇量固定）</strong>的，将当前任务的词汇表中的每个词编码成一个向量（<code>fixed embedding</code>）。如下图所示：</li>
</ul>
<p><img src="/pictures/image-20200709095552184.png" alt="image-20200709095552184"></p>
<p>那么如何进行 word embedding？word embedding做了什么，以及能做什么？</p>
<p>参考论文如下：</p>
<p><img src="/pictures/image-20200709104821019.png" alt="image-20200709104821019"></p>
<p>word embedding 类似于<strong>类比推理</strong>（analogy reasoning），通过将某一单词的one-hot 向量进行 word embedding 后（某些向量计算）可以得到另外一种向量表示（通常叫<code>Distributed Representation</code>），这种向量表示可以反映该单词的各方面特征，如：<code>Man：Woman as Boy：Girl</code>，这两组向量的<strong>距离</strong>（可以当成向量内积）不会差太远，因为表达的意思很相近。</p>
<p>通过上述描述，其实 word embedding的本质很接近于一个矩阵，可以称之为 Embedding matrix，通过将 one-hot 向量与该矩阵相乘（或者某些其他操作），即可得到另外一种好的向量表示。如下图所示：</p>
<p><img src="/pictures/image-20200709113108113.png" alt="image-20200709113108113"></p>
<p>那么如何学习 Embedding matrix？</p>
<p>参考论文如下：</p>
<p><img src="/pictures/image-20200709113343470.png" alt="image-20200709113343470"></p>
<p>通常我们是通过学习一个神经语言模型来得到比较好的 Embedding matrix的，如下所示：</p>
<p><img src="/pictures/image-20200709114623503.png" alt="image-20200709114623503"></p>
<h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p><strong>word2vec</strong> 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包，它简单、高效，因此引起了很多人的关注。word2vec 的作者 <code>Tomas Mikolov</code> 在下述两篇论文中对 word2vec进行了阐明：</p>
<ul>
<li>Efficient Estimation of Word Representations in Vector Space</li>
<li>Distributed Representations of Words and Phrases and their Compositionality</li>
</ul>
<p>word2vec的结构表示如下图所示：</p>
<p><img src="/pictures/word2vec.png" alt="word2vec"></p>
<p>前述有表达过词向量的两种表达形式，分别是：<code>One-Hot Representation</code> 和 <code>Distributed Representation</code>。据此再结合word2vec中的两个重要模型<strong>CBOW模型</strong>（Continuous Bag-of-Words Model）和<strong>Skip-gram模型</strong>（Continuous Skip-gram Model）即可对word2vec有了初步的了解。</p>
<p>两个模型表示如下述两图所示：前后分别为CBOW和Skip-gram。</p>
<p>由图可见，两个模型都包含三层：输入层、投影层和输出层。</p>
<script type="math/tex; mode=display">
前者是在已知当前词w_t的上下文，w_{t-2},w_{t-1},w_{t+1},w_{t+2}的前提下预测当前词w_t（见第一幅图）\\\\
而后者恰恰相反，是在已知当前词w_t的前提下，预测其上下文w_{t-2},w_{t-1},w_{t+1},w_{t+2}(见第二幅图)</script><p><img src="/pictures/CBOW.png" alt="CBOW"></p>
<p><img src="/pictures/Skip-gram.png" alt="Skip-gram"></p>
<h3 id="CBOW-和-Skip-gram"><a href="#CBOW-和-Skip-gram" class="headerlink" title="CBOW 和 Skip-gram"></a>CBOW 和 Skip-gram</h3><p>对于CBOW和Skip-gram两个模型，word2vec给出了两套框架，它们分别基于<code>Hierarchical Softmax</code> 和 <code>Negative Sampling</code> 来进行设计的。</p>
<ul>
<li>首先是基于<code>Hierarchical Softmax</code> 的CBOW 和 Skip-gram模型。</li>
</ul>
<p>一般以神经网络来设计的语言模型的目标函数通常取为如下的对数似然函数：</p>
<script type="math/tex; mode=display">
\Gamma = \sum_{w \in C}\log p(w|Context(w)) \\\\
其中的关键是条件概率函数p(w|Context(w))的构造 \\\\
在论文中给出了如下的构造形式：p(w|Context(w)) = \frac{e^{y_{w,i_w}}}{\sum_{i=1}^{N} e^{y_{w,i}}}</script><p>对于 word2vec 中基于 <code>Hierarchical Softmax</code> 的 <code>CBOW</code> 模型，优化的目标函数也跟上述对数似然函数相同；</p>
<p>而对于基于 <code>Hierarchical Softmax</code> 的 <code>Skip-gram</code> 模型，优化的目标函数则形如下式：</p>
<script type="math/tex; mode=display">
\Gamma = \sum_{w \in C}\log p(Context(w)|w)</script><script type="math/tex; mode=display">
由上述两个公式的对比可以知道，关于 CBOW 模型和 Skip-gram 模型关注的重点 \\\\
应该放在 p(w|Context(w)) 和 p(Context(w)|w)上。</script><h3 id="CBOW的详细表述"><a href="#CBOW的详细表述" class="headerlink" title="CBOW的详细表述"></a>CBOW的详细表述</h3><p>CBOW 包括三层：输入层、投影层和输出层。下面以样本(Context(w),w)为例(这里假设Context(w)由w前后各c个词构成)，对这三个层做简要说明。</p>
<p><strong>输入层</strong>：</p>
<script type="math/tex; mode=display">
包含Context(w)中 2c 个词的词向量 v(Context(w)_1),v(Context(w)_2),\ldots$ $,v(Context(w)_{2c}) \in  \mathbb{R}^m .这里，m的含义表示词向量的维度。</script><p><strong>投影层</strong>：</p>
<script type="math/tex; mode=display">
将输入层的 2c 个向量做求和累加，即:x_w = \sum_{i=1}^{2c} v(Context(w)_i)    \in   \mathbb{R}^m</script><p><strong>输出层</strong>：</p>
<p>输出层对应一颗二叉树，它是以语料中出现过的词当叶子结点，以各词在语料中出现的次数当权值构造出来的 Huffman 树。在这颗 Huffman 树中，叶子节点共 N个，其中 N=|D| ，分别对应词典D中的词，非叶子结点N-1个（图中标成黄色的结点）。</p>
<p>如下图所示：</p>
<p><img src="/pictures/CBOW1.png" alt="CBOW1"></p>
<p><strong>总结</strong>：</p>
<p>与神经网络语言模型相比较容易发现它们有三处不同：</p>
<ol>
<li><p>（从输入层到投影层的操作）前者是通过拼接，后者通过累加求和。</p>
</li>
<li><p>（隐藏层）前者有隐藏层，后者无隐藏层。</p>
</li>
<li><p>（输出层）前者是线性结构，后者是树形结构。</p>
</li>
</ol>
<p>在神经概率语言模型中，模型的大部分计算集中在隐藏层和输出层之间的矩阵向量运算，以及输出层上的softmax归一化运算，而从上面的对比中可见，CBOW模型对这些计算复杂度高的地方有针对性的进行了改变，首先，去掉了隐藏层，其次，输出层改用了Huffman树，从而为利用 <code>Hierarchical Softmax</code> 技术奠定了基础。</p>
<h3 id="Skip-gram-模型的详细描述"><a href="#Skip-gram-模型的详细描述" class="headerlink" title="Skip-gram 模型的详细描述"></a>Skip-gram 模型的详细描述</h3><p>下图给出了 Skip-gram 模型的网络结构，同 CBOW模型的网络结构一样，它也包括三层：输入层、投影层和输出层。下面以样本 (w,Context(w))为例，对这三个层做简要说明。</p>
<p><strong>输入层</strong>：</p>
<script type="math/tex; mode=display">
只含有当前样本的中心词 w 的词向量 v(w) \in  \mathbb{R}^m.</script><p><strong>投影层</strong>：</p>
<p>这是个恒等投影，把 v(w) 投影到 v(w) 上。因此，这个投影层其实有点多余，这里之所以保留投影层主要是方便和 CBOW模型的网络结构做对比。</p>
<p><strong>输出层</strong>：</p>
<p>和CBOW模型一样，输出层也是一颗 Huffman 树.</p>
<p><img src="/pictures/Skip-gram1.png" alt="Skip-gram1"></p>
<h3 id="Negative-sampling"><a href="#Negative-sampling" class="headerlink" title="Negative sampling"></a>Negative sampling</h3><p>先定义训练集，如下图所示：</p>
<p><img src="/pictures/image-20200710103828084.png" alt="image-20200710103828084"></p>
<p>然后定义训练模型：每次训练都只是训练其中的 k+1个二分类器。不再计算一个 10,000维度的softmax分类器，而是转化为了10,000个二分类问题，如下图所示：</p>
<p><img src="/pictures/image-20200710105028651.png" alt="image-20200710105028651"></p>
<p>最后的细节就是如何选取负样本，如果随机均匀选取，明显会不太合理，因此采用经验公式，来估计英文单词的分布概率，如下图所示：</p>
<p><img src="/pictures/image-20200710104613615.png" alt="image-20200710104613615"></p>

    </div>

    
    
    
        <div class="reward-container">
  <div>觉着不错，打赏一下吧。</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/pictures/wechatpay.jpg" alt="刘涛 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/pictures/alipay.jpg" alt="刘涛 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>刘涛
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://lt4hyl.top/2020-09-02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89.html" title="深度学习笔记（5）">http://lt4hyl.top/2020-09-02/深度学习笔记（5）.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" rel="tag"># 基础知识</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020-08-28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%884%EF%BC%89.html" rel="prev" title="深度学习笔记（4）">
      <i class="fa fa-chevron-left"></i> 深度学习笔记（4）
    </a></div>
      <div class="post-nav-item">
    <a href="/2020-09-02/%E5%B0%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB.html" rel="next" title="小目标检测论文阅读">
      小目标检测论文阅读 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Sequence-Models"><span class="nav-number">1.</span> <span class="nav-text">Sequence Models</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E7%9A%84%E5%BC%95%E5%85%A5"><span class="nav-number">1.1.</span> <span class="nav-text">RNN的引入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Language-Model-and-Sequence-generation"><span class="nav-number">1.2.</span> <span class="nav-text">Language Model and Sequence generation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88Language-Model-%EF%BC%89"><span class="nav-number">1.3.</span> <span class="nav-text">语言模型（Language Model ）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gated-Recurrent-Unit-GRU"><span class="nav-number">1.4.</span> <span class="nav-text">Gated Recurrent Unit(GRU)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Long-Short-Term-Memory-LSTM"><span class="nav-number">1.5.</span> <span class="nav-text">Long Short-Term Memory(LSTM)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM%E6%98%AF%E5%A6%82%E4%BD%95%E5%87%8F%E8%BD%BB%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98%E7%9A%84"><span class="nav-number">1.6.</span> <span class="nav-text">LSTM是如何减轻梯度消失问题的?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bidirectional-RNN"><span class="nav-number">1.7.</span> <span class="nav-text">Bidirectional RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deep-RNN"><span class="nav-number">1.8.</span> <span class="nav-text">Deep RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Word-Representation%EF%BC%88%E8%AF%8D%E6%B1%87%E8%A1%A8%E5%BE%81%EF%BC%89"><span class="nav-number">1.9.</span> <span class="nav-text">Word Representation（词汇表征）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#word2vec"><span class="nav-number">1.10.</span> <span class="nav-text">word2vec</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CBOW-%E5%92%8C-Skip-gram"><span class="nav-number">1.10.1.</span> <span class="nav-text">CBOW 和 Skip-gram</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CBOW%E7%9A%84%E8%AF%A6%E7%BB%86%E8%A1%A8%E8%BF%B0"><span class="nav-number">1.10.2.</span> <span class="nav-text">CBOW的详细表述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Skip-gram-%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%A6%E7%BB%86%E6%8F%8F%E8%BF%B0"><span class="nav-number">1.10.3.</span> <span class="nav-text">Skip-gram 模型的详细描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Negative-sampling"><span class="nav-number">1.10.4.</span> <span class="nav-text">Negative sampling</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">刘涛</p>
  <div class="site-description" itemprop="description">为人民日益增长的美好生活需要而读书：机器学习、深度学习、c++、python、C#</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">34</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">京ICP备2020034167号-1 </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">刘涛</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">165k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">2:30</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;
      
      
      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          //console.log(error);
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"MxrUXwts7rlzq762NfLCiSVk-gzGzoHsz","app_key":"bqlnUht41VcOv8c5hK8DrOyz","server_url":"https://mxruxwts.lc-cn-n1-shared.com","security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@2.7.8/unpacked/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
