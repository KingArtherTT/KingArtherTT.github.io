<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Lancelot的小站</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="1.上采样(upsampling) 下采样(subsampled)  :  缩小图像（或称为下采样（subsampled）或降采样（downsampled））  放大图像（或称为上采样（upsampling）或图像插值（interpolating））  下采样原理：对于一幅图像I尺寸为MN，对其进行s倍下采样，即得到(M&#x2F;s)(N&#x2F;s)尺寸的得分辨率图像，当然s应该是M和N的公约数才行，如果考虑的">
<meta property="og:type" content="article">
<meta property="og:title" content="Lancelot的小站">
<meta property="og:url" content="http://lt4hyl.top/2020-08-14/CV.html">
<meta property="og:site_name" content="Lancelot的小站">
<meta property="og:description" content="1.上采样(upsampling) 下采样(subsampled)  :  缩小图像（或称为下采样（subsampled）或降采样（downsampled））  放大图像（或称为上采样（upsampling）或图像插值（interpolating））  下采样原理：对于一幅图像I尺寸为MN，对其进行s倍下采样，即得到(M&#x2F;s)(N&#x2F;s)尺寸的得分辨率图像，当然s应该是M和N的公约数才行，如果考虑的">
<meta property="og:locale">
<meta property="og:image" content="http://lt4hyl.top/2020-08-14/pictures/image-20200612163951238.png">
<meta property="og:image" content="http://lt4hyl.top/2020-08-14/pictures/image-20200618190132230.png">
<meta property="article:published_time" content="2020-08-14T04:24:48.766Z">
<meta property="article:modified_time" content="2020-06-27T14:57:35.547Z">
<meta property="article:author" content="刘涛">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://lt4hyl.top/2020-08-14/pictures/image-20200612163951238.png">
  
    <link rel="alternate" href="/atom.xml" title="Lancelot的小站" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.0.2"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Lancelot的小站</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://lt4hyl.top"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-CV" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020-08-14/CV.html" class="article-date">
  <time datetime="2020-08-14T04:24:48.766Z" itemprop="datePublished">2020-08-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>1.上采样(upsampling) 下采样(subsampled)  :</p>
<p> 缩小图像（或称为下采样（subsampled）或降采样（downsampled））</p>
<p> 放大图像（或称为上采样（upsampling）或图像插值（interpolating））</p>
<p> <strong>下采样原理：</strong>对于一幅图像I尺寸为M<em>N，对其进行s倍下采样，即得到(M/s)</em>(N/s)尺寸的得分辨率图像，当然s应该是M和N的公约数才行，如果考虑的是矩阵形式的图像，就是把原始图像s*s窗口内的图像变成一个像素，这个像素点的值就是窗口内所有像素的均值.</p>
<p><strong><em>上采样原理：</em></strong>图像放大几乎都是采用内插值方法，即在原有图像像素的基础上在像素点之间采用合适的插值算法插入新的元素。(通常有使用双线性内插值和三线性内插值)</p>
<p>2.过采样（oversample）</p>
<h2 id="TP-TN-FP-FN"><a href="#TP-TN-FP-FN" class="headerlink" title="TP/TN/FP/FN"></a>TP/TN/FP/FN</h2><ul>
<li>FN：False Negative,被判定为负样本，但事实上是正样本。</li>
<li>FP：False Positive,被判定为正样本，但事实上是负样本。</li>
<li>TN：True Negative,被判定为负样本，事实上也是负样本。</li>
<li>TP：True Positive,被判定为正样本，事实上也是证样本。</li>
</ul>
<p>首先这几个术语会高频率得出现在论文的实验部分，它是对实验结果的描述，首先我想先解释这几个缩写的含义：<br>precesion：查准率，即在检索后返回的结果中，真正正确的个数占整个结果的比例。<br>recall：查全率，即在检索结果中真正正确的个数 占整个数据集（检索到的和未检索到的）中真正正确个数的比例。</p>
<p>这里的正样本和负样本与检索的关系就是：你认为为正样本的应该都出现在检索结果中，而你认为为负样本的应该不出现在检索结果中，但是你认为的和事实上的会有不一样。</p>
<p>这里的四个缩写曾经一度让我很难记住，经过细想，发现这样比较好记忆：把缩写分为两个部分，第一个字母（F,T）和第二个字母（P,N）。首先搞清楚第二个字母，即它是你认为该样本的归属应该是怎样（Positive or Negative）；第一个字母即是对你的判断进行的评价（False or True）。这里也许中文可能会有不好理解的地方，所以我想用英文来描述，可能更清晰：第二个字母：What’s your judgement about the sample?；第一个字母：Is your judgement right(true) or not(false)?</p>
<p>那么有：<br>precesion = TP/(TP+FP) 即，检索结果中，都是你认为应该为正的样本（第二个字母都是P），但是其中有你判断正确的和判断错误的（第一个字母有T ，F）。</p>
<p>recall = TP/(TP+FN)即，检索结果中，你判断为正的样本也确实为正的，以及那些没在检索结果中被你判断为负但是事实上是正的（FN）。</p>
<h2 id="SNIP"><a href="#SNIP" class="headerlink" title="SNIP"></a>SNIP</h2><p>Scale Normalization for Image Pyramids</p>
<p>图像金字塔的比例尺归一化</p>
<p>一个训练方案</p>
<p>有选择性的进行不同尺寸的目标实例的后向梯度传播，作为一个图像尺度的函数。</p>
<p>使用了现成的 ImageNet-1000 预训练模型，并且只进行了边界框的有监督训练。</p>
<p>跨对象实例的大尺度变化，尤其是小目标检测的挑战是性能差异背后的因素之一。</p>
<p>由于ImageNet  和 Coco 数据 目标实例尺度的差异较大，在用 预训练的分类网络模型 在 Coco上进行 Fine-tune时，会导致 domain-shift的问题。</p>
<p>已有的解决方案，用于减轻 尺度变化和小目标检测所引起的问题：</p>
<ol>
<li>将浅层特征（靠近输入端的特征）与深层特征结合起来用于检测小目标。</li>
<li>dilated/deformable convolution 用于增大感受野，用来检测大目标。</li>
<li>不同分辨率的层 ，进行独立预测用于捕获不同尺度的目标实例。</li>
<li>上下文（Context）用于消除歧义。</li>
<li>在一系列的尺度上进行训练；在图像金字塔的多尺度上进行推理；将非极大值抑制与预测相结合起来。</li>
</ol>
<p>SNIP有益于在训练中减少尺度变化，但对于减少的训练样本没有付出相应的罚金（或者说是减少的训练样本，没有对训练产生衣橱。）</p>
<p>使用图像金字塔实现了尺度不变性，代替了尺度不变的检测器。这包含了归一化输入目标实例的表示在图像金字塔的某一个尺度上。</p>
<p>在训练中为了最小化 domain shift，仅仅只对  ROIs/anchors 在分辨率上接近于预训练CNN模型的 进行后向传播。</p>
<p>尽管特征金字塔可以有效的利用来自网络不同层的特征，但通过FPN产生的高级别的语义特征会不利于检测特别小或者特别大的目标。</p>
<p>我们观察到在 R-FCN中添加 针对每个类别的缩放固定的过滤器（尺度固定的过滤器）会降低用于目标检测性能。</p>
<p>由于在ImageNet上进行预训练是有益的，并且在较大的目标实例上学习的过滤器有助于对较小的目标实例进行分类，因此，对图像进行上采样和使用在高分辨率图像上进行预训练的网络应该比使用用于小目标分类的专用网络更好。</p>
<p>SNIP 强调使用在同一网络上使用<strong>上采样</strong>而不是使用不同的网络架构来检测小目标。</p>
<p>SNIP 使用了  Deformable-RFCN 作为检测网络</p>
<p>在训练阶段和测试阶段不同的图像分辨率回导致性能出现显著的下降。不幸的是，受限于GPU内存，分辨率的不同是现在目标检测的一部分。（训练采用低分辨率，测试采用高分辨率）。</p>
<p>在更高分辨率尺度的图像上训练检测器，可以提升对小目标分分类性能。但会 摧毁中到大型的目标，这会降低性能。</p>
<p>SNIP试验后总结到：使用合适尺度的目标来训练检测器是很重要的；同时应该尽可能多的捕获目标间的变化（不同尺度，姿态等）</p>
<p>MST（Multi-Scale Training）：训练期间在不同分辨率的图像上进行随机采样，然后将样本用于训练。</p>
<p>SNIP是MST（Multi-Scale Training）的一个修改版本，其中用于训练的目标实例都具有与预训练数据集相似的分辨率，这通常是 224 X 224.</p>
<p>在MST中，在较大分辨率上<strong>大的目标</strong>难以分类；在较低分辨率上<strong>小的目标</strong>难以分类。幸运的是，每个目标实例都有好几个不同的尺度，其中的一些会落在我们需要的尺度范围内。为了消除极端尺度目标的影响，MST只会将在需要的尺度范围内的目标用于后向传播，其余的都直接忽略掉。</p>
<h3 id="SNIP的实现细节："><a href="#SNIP的实现细节：" class="headerlink" title="SNIP的实现细节："></a>SNIP的实现细节：</h3><ol>
<li>训练分类器时，所有的GT被用于分配标签来进行 Proposals。</li>
<li>在训练过程中，不会选择超出特定尺寸范围且超出特定大小分辨率的  proposal 和 gt（真实框）</li>
</ol>
<p><img src="pictures/image-20200612163951238.png" alt="image-20200612163951238"></p>
<p>核心观点有二：</p>
<ol>
<li><p>是对参与训练的目标实例进行筛选，剔除了过大或者过小的目标，将与预训练网络所用数据集分辨率相似的目标实例用于训练。降低了 domain shif问题。</p>
</li>
<li><p>是针对有小目标的图片的<strong>数据增强</strong>（ Sampling Sub-Images）：先将原图进行上采样，然后进行随机裁剪，获取包含小目标的子样本，将其添加到训练集中。</p>
</li>
</ol>
<p>网络主体使用的是   Deformable-RFCN</p>
<h2 id="SNIPER"><a href="#SNIPER" class="headerlink" title="SNIPER"></a>SNIPER</h2><p>chips：真实框附近的上下文区域。</p>
<p>chips： scale specific context-regions ；</p>
<p>SNIPER最终处理的chips的分辨率是 512*512.</p>
<p>主体采用的是 Faster-RCNN，backbone是 ResNet-101</p>
<p>由于计算量大的原因，在许多情景下使用多尺度训练是不切实际的。</p>
<p>在使用多尺度训练时，忽略过大或者过小分辨率的目标的梯度是有益。</p>
<p>Scale Normalization for Image Pyramids with Efficient Resampling (SNIPER)</p>
<p>该方法以图像内容为条件，适应性的从多尺度图像金字塔中采样 chips。以真实框为条件，采样 positive chips；以RPN网络的 proposals 来采样 negative chips。</p>
<p>SNIPER在 512 X 512 size 的 chips 上训练，这可以从 较大的 batch size 中获得益处，并且在训练中可以使用 批归一化（BN）在单GPU节点上。512 X 512 的 chips 从 480px，800px，1000px 这三个尺度的图像金字塔中采样获得。</p>
<p> false positive rate 误报率</p>
<p>由于我们可以通过 观察不到十分之一的图像来获得与全部图像相似的性能，这表明在训练高性能检测器中较大的上下文信息不再重要了，但是在区域采样中包含 hard negatives 样本反而更加重要。</p>
<p>SNIPER 核心观点有两个：</p>
<ol>
<li>positive/negative chip 挖掘</li>
<li>另外一个是对生成的 chips 的标签的分配</li>
</ol>
<p>对于高分辨率的图像，SNIPER不处理大部分的背景，这可以节省算力和显存，当处理高分辨率图像时。</p>
<p>negative chip sample 针对的是较容易分类的背景。不正确的背景分类会导致误报率的上升。在当前 多尺度训练中，图像的每个像素都需要被计算，尽管这可以降低误报率，但是会提升计算量。</p>
<p>因此我们应该怎么避免对这些区域的全部计算，而尽可能的消除这些容易分类的背景区域。一个简单的方法是应用 object proposal（RPN）来区分在图像的哪些区域目标更容易出现。如果在一张图片的某部分上没有 proposal 被提出，这就表明，这张图的这部分很容易的被分类为背景，这样在训练中，我们就可以忽略这张图的这部分内容。</p>
<p>SNIPER 只处理那些更可能包含 false positive的区域。</p>
<h2 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h2><p>首先残差连接解决了两个问题：</p>
<ol>
<li><p>是解决了训练深层网络是梯度消失的问题。</p>
<p>skip connect 使得链式求导，反向传播进行参数更新时，每一个导数都加上了一个恒等项 1，使得误差可以有效的反向传播。</p>
</li>
<li><p>是改善了随着网络的加深，权重矩阵的退化问题。</p>
<p>退化问题是指：每个层中只有少量的隐藏单元对不同的输入改变它们的激活值，而大部分隐藏单元对不同的输入都是相同的反应。此时整个权重矩阵的秩不高。并且随着网络层数的增加，连乘后使得整个秩变的更低。</p>
<p>这样即便权重矩阵是一个很高维的矩阵，但是大部分维度却没有信息，表达能力没有看起来那么强大。</p>
</li>
</ol>
<p><strong>残差连接 正式强制打破了网络的对称性。</strong></p>
<h3 id="残差连接的思想："><a href="#残差连接的思想：" class="headerlink" title="残差连接的思想："></a>残差连接的思想：</h3><p>正常网络可以使用一个非线性变化函数来描述，定义为 输入为 X，输出为 F(X)，F包含了网络的卷积、激活等操作。</p>
<p>当我们强行将一个输入添加到函数的输出时，可以用 G(X) 来表达输出， <strong>G(X) = F(X) + X</strong>;F(x)和X的线性叠加。</p>
<p><strong>这就是 skip connect的思想，将输出表述为输入和输入的一个非线性变化的线性叠加。</strong></p>
<p>残差结构来源于LSTM的控制门思想。</p>
<p>实际使用时，是通过一个下采样操作，将输入直接加入到输出中。</p>
<p><img src="pictures/image-20200618190132230.png" alt="image-20200618190132230"></p>
<h2 id="FSSD：Feature-Fusion-Single-Shot-Multibox-Detector"><a href="#FSSD：Feature-Fusion-Single-Shot-Multibox-Detector" class="headerlink" title="FSSD：Feature Fusion Single Shot Multibox Detector"></a>FSSD：Feature Fusion Single Shot Multibox Detector</h2><ul>
<li><p>提出了一个新的 feature fusion module</p>
</li>
<li><p>对于以 VGG16 为backbone 的SSD，以Conv4_3，放缩8倍后用于检测小目标，以 Conv8_2 放缩64倍后检测大目标。这个策略是合理的，因为小目标在浅层特征中不会丢失太多的位置信息，大目标在深层特征可以被很好的定位和识别。</p>
<ul>
<li>但问题在于浅层特征缺少足够的语义信息用于小目标分类。</li>
<li>与此同时，小目标会严重依赖于上下文信息用于检测和分类。</li>
</ul>
</li>
<li><p>FSSD为了解决上述问题，在传统的SSD中新增了一个轻量的有效的  feature fusion module。</p>
</li>
<li><p>来自不同层拥有不同尺度的特征通过 Batch Normalization层 投影并连接到一起，归一化特征值。然后添加了一些下采样区块用于产生新的特征金字塔，讲他们喂给 multibox detectors 用于产生最后的检测结果。</p>
</li>
<li><p>SSD 将不同级别产生的特征视作是同一级别的，并直接从这些特征中产生目标检测结果。这个策略使得SSD 缺少能力去同时获取局部细节特征和全局语义特征。</p>
</li>
<li><p>然而检测器应该整合上下文特征和他们详细的特征用于确认小目标。因此采用一个轻量的架构合成这些特征是一个重要的解决方案用于卷积网络目标检测器提升检测精度。</p>
</li>
<li><p>FSSD的目的是：采用合适的方式一次融合不同级别的特征并从融合特征中产生图像金字塔。</p>
<ul>
<li>设计时要考虑的因素：</li>
<li>用于融合的源 fanture map：conv3_3（后来舍弃了），conv4_3,fc_7,conv7_2（将conv6_2的步长设置为1）.作者认为feature map 的size 小于10 x 10 的由于包含信息少，因此不用merge。</li>
<li>feature fusion function：有两种主要的方式用 merge 不同的 feature maps： concatenation and element-wise summation。采用的是 concatenation <ul>
<li>其中 element-wise summation 需要feature map 的size保持一致，channel保持一致，确少灵活性，因此没有采用。</li>
<li>做过实验后 concatenation 的效果要比 element-wise summation 好。</li>
</ul>
</li>
<li>transformation function：在各个 source feature map    进行 concatenated together 之前的转换函数。<ul>
<li>首先采用 1 x 1的卷积来降维。</li>
<li>然后将 conv4_3 的 feature map size 作为基准（38*38）这也就意味着最小的 feature step 是8.（放缩了8倍）。</li>
<li>对于 conv3_3 采用 max pooling 进行下采样缩放到 38*38</li>
<li>对于size更小的层，使用双线性插值（ bilinear interpolation）重新缩放到 38*38</li>
</ul>
</li>
<li>用于产生特征金字塔的函数：比较了三种不同的形式，最终采用的是：simple block after fusion feature。</li>
</ul>
</li>
</ul>
<h2 id="Feature-Fused-SSD-Fast-Detection-for-Small-Objects"><a href="#Feature-Fused-SSD-Fast-Detection-for-Small-Objects" class="headerlink" title="Feature-Fused SSD: Fast Detection for Small Objects"></a>Feature-Fused SSD: Fast Detection for Small Objects</h2><ul>
<li>采用了SSD，提出了一个 多级别的 融合特征方法用于在SSD中引入上下文信息，提升对于小目标检测的准确性。</li>
<li>在详细的融合操作上，作者设计了两个融合特征模块，用于添加上下文信息：concatenation module and element-sum module。<ul>
<li>concatenation module：使用 1 x 1得卷积层用于学习目标信息和上下文信息相融合得权重。这可以减少无用的背景噪音。</li>
<li>element-sum module：使用手动设置的相同的权重来融合多等级的特征以一种必须的方式，这可以有效的增强上下文信息的重要性。</li>
</ul>
</li>
<li>目标在于 对小目标进行更快的检测。</li>
</ul>
<h2 id="MultiResolution-Attention-Extractor-for-Small-Object-Detection"><a href="#MultiResolution-Attention-Extractor-for-Small-Object-Detection" class="headerlink" title="MultiResolution Attention Extractor for Small Object Detection"></a>MultiResolution Attention Extractor for Small Object Detection</h2><ul>
<li><p>现有的小目标检测方法主要聚焦在数据预处理和降低大目标和小目标之间的不同。受到了人类视觉“注意力”机制的启发，本文利用两个特征抽取方法来获得有用的小目标信息。两种方法都基于多分辨率的特征抽取。</p>
</li>
<li><p>本文设计了一个高质量的特征提取器用于不同系统的小目标的检测与分割。 MultiResolution Attention Extractor(MRAE)。吸纳了“注意力机制”，聚焦在更有用的特征上。</p>
</li>
<li><p>Self-attention、internal attention、domain attention、 feature-based channel-wise attention mechanism</p>
</li>
<li><p>MRAE 强调原始的ResNet不同级别上，最有用的 feature maps，并且也进行了特征融合进一步的提升有用的信息。不同层级上的feature map 是进行了权重相加，其中的注意力权重由一个小的网络学习得到（一个卷积层、一个Fc层，cosine相似度操作和上采样操作）后续接着一个softmax层。由这个网络组成了新的 feature map 叫做 attention maps。</p>
</li>
<li><p>MRAE 通过学习到的注意力权重，在融合不同分辨率的特征时，降低了无用特征的信息（较低的注意力权重），并强调了最有用的特征信息（更高的注意力权重）。</p>
<ul>
<li>Soft attention：从原有的ResNet101中抽取三个层级的feature map（conv2, conv3, conv4）分别输入另外一个小的网络，用于获取注意力权重。注意力权重被定义为：用于每个层级特征并生成最后 attention map的权重。这个小网络的最终输出，可以理解成为一个单独的数值，后续进入softmax，变成概率数值，然后与原始的 feature map 直接相乘，再将这些 feature map 直接加到用于预测输出的feature map上，构建成最终的attention map，通过这种形式抑制不需要的信息，增强有用的信息。</li>
<li>Attention-based feature interaction MRAE：也是通过从原有的ResNet101中抽取三个层级的feature map（conv2, conv3, conv4）分别输入到一个小的网络，小的网络通过 1x1的卷积和 fc层，把feature map 映射成 vector后，以其中一个层级的vector为基准，计算与它的余弦相似度，这个余弦相似度就是用于输出的“注意度”。后续操作跟Soft attention类似。</li>
</ul>
</li>
<li><p>没有代码实现，但是是基于Faster R-CNN，并且不是在整个 CoCo数据集上做训练和测试，而是在选出来的小目标上做的训练和测试，可以尝试复现这种结构，然后在整个Coco数据集上做训练和测试。并于SNIPER的思想相叠加。</p>
</li>
</ul>
<h2 id="HRDNet-High-resolution-Detection-Network-for-Small-Objects"><a href="#HRDNet-High-resolution-Detection-Network-for-Small-Objects" class="headerlink" title="HRDNet: High-resolution Detection Network for Small Objects"></a>HRDNet: High-resolution Detection Network for Small Objects</h2><ul>
<li>给网络输入 高分辨率的图片有利于解决小目标随着网络加深小目标本身的信息会逐渐消失的问题。<ul>
<li>然而简单的增大分辨率会造成许多问题：增大了原本数据集的尺度变化幅度，并且引入了更大的计算量。</li>
</ul>
</li>
<li>High-Resolution Detection Network (HRDNet)，保留了高分辨率图片的好处并且不带来新的问题。<ul>
<li>提出了：Multi-Depth Image Pyramid Network (MD-IPN) ；  Multi-Scale Feature Pyramid Network (MS-FPN)。为了有效的利用 multiple features。</li>
<li>MD-IPN：维持了多个位置信息，使用了多个深度backbones；<ul>
<li>尤其是：高分辨率的图片将被输入给浅层网络，用于保留更多的位置信息和降低计算量</li>
<li>低分辨率的图片用于输入深层的网络来抽取更多的语义信息。</li>
</ul>
</li>
<li>MS-FPN：用于对齐和融合 多尺度的特征组（由MD-IPN产生）减少在这些多尺度、多层级特征之间的信息失衡。</li>
</ul>
</li>
<li>核心观点：使用深层的 backbone来处理低分辨率图片，同时使用浅层的 backbone 来处理高分辨率图片。</li>
<li>最后总结：本文使用深层的 backbone来处理低分辨率图片，同时使用浅层的 backbone 来处理高分辨率图片。这种设计真的是很不错，解决了语义信息与位置信息的不平衡的问题。可以进一步在对 feature map的融合上进行深入探索，也就是对MS-FPN网络的改进上，如何更加有效的利用来自 浅层网络的位置信息和深层网络的语义信息。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://lt4hyl.top/2020-08-14/CV.html" data-id="ckdtq9wye00014ku30bpkhxse" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2020-08-14/hello-world.html" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">August 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020-08-14/CV.html">(no title)</a>
          </li>
        
          <li>
            <a href="/2020-08-14/hello-world.html">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 刘涛<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>