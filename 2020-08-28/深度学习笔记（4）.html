<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.0.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lt4hyl.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Computer Vision（计算机视觉）Convolutional Neural Networks 随着感知向量机层数的加深，逐渐进入了深层感知向量机也可以叫做 深度神经网络（DNN）； DNN在处理图片数据时会引发参数灾难 DNN的隐层之间都是采用全连接的形式，假设图片较大，比如：300x300x3（宽高均为300像素，红蓝绿三通道）； 此时假设DNN某一隐层的输入等于图片大小，输出也想等于">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习笔记（4）">
<meta property="og:url" content="http://lt4hyl.top/2020-08-28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%884%EF%BC%89.html">
<meta property="og:site_name" content="Lancelot的小站">
<meta property="og:description" content="Computer Vision（计算机视觉）Convolutional Neural Networks 随着感知向量机层数的加深，逐渐进入了深层感知向量机也可以叫做 深度神经网络（DNN）； DNN在处理图片数据时会引发参数灾难 DNN的隐层之间都是采用全连接的形式，假设图片较大，比如：300x300x3（宽高均为300像素，红蓝绿三通道）； 此时假设DNN某一隐层的输入等于图片大小，输出也想等于">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://lt4hyl.top/pictures/CNNjuanjicaozuo.gif">
<meta property="og:image" content="http://lt4hyl.top/pictures/cnnfull.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/cnnsame.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/cnnvalid.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200703223034497.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200703232550227.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200703232334195.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/cnnpooling.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200704103937470.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200705145504908.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200705150520999.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200705151340754.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200705160052849.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200705162410718.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200705174817616.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200705175728639.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200705175906241.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200705180252143.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200705185043775.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200705190431839.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200705190546428.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200705203127470.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200705210421996.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200705211422908.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706113150576.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706114658214.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706121231188.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706135247615.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706142443842.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706152516446.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706153241763.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706153851813.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706154904663.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706200918442.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706202152963.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706203216698.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706203255638.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/cnnsiameseloss.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706205851202.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706210610393.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706212002904.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706212455827.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706214025099.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/cnngrammatrix.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/cnnstyletransfer.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706214830502.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706215315522.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706221906851.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706221800278.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200706223240303.png">
<meta property="article:published_time" content="2020-08-28T04:26:45.000Z">
<meta property="article:modified_time" content="2020-09-02T04:24:11.190Z">
<meta property="article:author" content="刘涛">
<meta property="article:tag" content="基础知识，深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://lt4hyl.top/pictures/CNNjuanjicaozuo.gif">

<link rel="canonical" href="http://lt4hyl.top/2020-08-28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%884%EF%BC%89.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>深度学习笔记（4） | Lancelot的小站</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?43b02ed86a701dce319ff86325707452";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Lancelot的小站</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">记录点滴成长：AI所向，吾之所往</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://lt4hyl.top/2020-08-28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%884%EF%BC%89.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="刘涛">
      <meta itemprop="description" content="为人民日益增长的美好生活需要而读书：机器学习、深度学习、c++、python、C#">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lancelot的小站">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习笔记（4）
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-08-28 12:26:45" itemprop="dateCreated datePublished" datetime="2020-08-28T12:26:45+08:00">2020-08-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-09-02 12:24:11" itemprop="dateModified" datetime="2020-09-02T12:24:11+08:00">2020-09-02</time>
              </span>

          
            <span id="/2020-08-28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%884%EF%BC%89.html" class="post-meta-item leancloud_visitors" data-flag-title="深度学习笔记（4）" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>11k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>10 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Computer-Vision（计算机视觉）"><a href="#Computer-Vision（计算机视觉）" class="headerlink" title="Computer Vision（计算机视觉）"></a>Computer Vision（计算机视觉）</h1><h2 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h2><ul>
<li>随着感知向量机层数的加深，逐渐进入了深层感知向量机也可以叫做 <strong>深度神经网络（DNN）</strong>；</li>
<li>DNN在处理图片数据时会引发<strong>参数灾难</strong><ul>
<li>DNN的隐层之间都是采用<strong>全连接</strong>的形式，假设图片较大，比如：300x300x3（宽高均为300像素，红蓝绿三通道）；</li>
<li>此时假设DNN某一隐层的输入等于图片大小，输出也想等于图片的大小，则该层的参数数量就为：<code>300x300x3x300x300x3=72,900,000,000</code>，也就7百万的参数量，这将导致网络无法训练，也就是常说的<strong>参数灾难</strong>或者<strong>维度灾难</strong>。</li>
<li>因此需要引入其它的处理方式，降低参数量的同时还能有效的学习到图片中包含的信息，也就是<strong>CNN：卷积神经网络</strong>。</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h3 id="卷积操作、padding、stride"><a href="#卷积操作、padding、stride" class="headerlink" title="卷积操作、padding、stride"></a>卷积操作、padding、stride</h3><ul>
<li>卷积操作如下图所示：即以一固定矩阵（卷积核）在原始输入矩阵（输入图像）上，按照一定规则滑动，在每次滑动的窗口中计算逐元素相乘的累加和，并将该值作为这个窗口的特征值。<ul>
<li>注：kernel/filter 是一回事儿，均指卷积核。</li>
</ul>
</li>
</ul>
<p><img src="/pictures/CNNjuanjicaozuo.gif" alt="CNNjuanjicaozuo"></p>
<ul>
<li>数学上的卷积操作（卷积核在与矩阵元素相乘前，有横轴和竖轴上的翻转操作）与 DL 上的卷积操作（卷积核没有翻转操作，数学上叫做 cross-correlation—交叉相关）略有不同。</li>
<li>这里的卷积操作跟传统图像处理中的 Harr特征提取过程其实挺像。<ul>
<li>区别在于：Harr特征提取时，我们手工设定了卷积核上的参数值，而在CNN中可以通过学习得到卷积核上的参数值；Harr特征通常只有一次卷积操作，而CNN中会通过 pooling、激活函数等操作叠加多个卷积操作。</li>
</ul>
</li>
</ul>
<ul>
<li><p><code>Padding</code>操作的引入：</p>
<ul>
<li>从上图演示的卷积操作中可以看出，每次卷积操作后，从原始输入得到的输出都会小一些；</li>
<li>而为了不希望图片随着每次卷积操作逐渐缩小，引入了带<code>padding</code>的卷积操作。</li>
<li>除此之外：图片边界上的像素点被使用的次数会很少，图片中心像素点被使用的次数会很多；</li>
<li>即图片边界上的信息会被丢失，为了不丢失这些信息，也需要引入<code>padding</code>操作。</li>
<li>padding 操作可以理解为在原始图片四周进行了像素填充，使得进行卷积操作的图像可以比原始图像稍微大一些。</li>
</ul>
</li>
<li><p>根据 padding实现形式的不同可以将卷积操作分为：full、same和valid三种形式。（这里假设所有stride都是1，也就是卷积核逐元素移动）</p>
<ul>
<li>要注意：现在又多了一种卷积形式并且应用广泛，就是：空洞卷积（<code>atrous convolutions</code>）又名扩张卷积（<code>dilated convolutions</code>），向卷积层引入了一个称为 “<code>扩张率(dilation rate)</code>”的新参数，该参数定义了卷积核处理数据时各值的间距。</li>
<li>full模式：从filter和image刚相交处开始做卷积（相当于进行卷积核<code>size-1</code>的边缘填充，此时<code>p=size-1</code>），白色部分可填充为0，其中橙色部分是image，蓝色部分为filter。filter的运动范围如下图所示：</li>
</ul>
<p><img src="/pictures/cnnfull.png" alt="cnnfull" style="zoom:50%;" /></p>
<ul>
<li>same模式：让该层输入输出的特征图大小保持不变，当filter的中心(K)与image的边角重合时（相当于进行卷积核<code>size/2</code>的边缘填充，此时<code>p=size/2向下取整</code>），开始做卷积运算，如下图示：</li>
</ul>
<p><img src="/pictures/cnnsame.png" alt="cnnsame" style="zoom:50%;" /></p>
<ul>
<li>valid模式：当filter全部在image里面的时候（相当于没有任何填充，此时<code>p=0</code>），进行卷积运算，如下图示：</li>
</ul>
<p><img src="/pictures/cnnvalid.png" alt="cnnvalid" style="zoom:50%;" /></p>
</li>
<li><p>stride的引入：</p>
<ul>
<li>前述卷积操作都是假设<code>stride=1</code>，也就是卷积核每次移动的步长为1，每次卷积核整体移动一个元素的距离。</li>
<li>但若要设计卷积操作输出的特征图的大小时，就需要考虑<code>stride=1</code>是否合适。</li>
</ul>
</li>
<li><p>卷积操作中指定了padding、stride和filter（卷积核）大小后，据此就可以根据该卷积层的输入特征图的大小计算出该层输出特征图的大小。</p>
<ul>
<li>计算公式如下图所示：</li>
</ul>
<p><img src="/pictures/image-20200703223034497.png" alt="image-20200703223034497"></p>
</li>
</ul>
<h3 id="卷积层参数量计算"><a href="#卷积层参数量计算" class="headerlink" title="卷积层参数量计算"></a>卷积层参数量计算</h3><ul>
<li>某一卷积层可以如下图所示：<ul>
<li><strong>6x6x3</strong>的输入经过了2个<strong>3x3x3</strong>的卷积核（也就是整体上是3x3x6的卷积核），其中padding设置为valid，即p=0，设置stride=1，后得到<strong>4x4</strong>的输出特征图加上偏置参数（bias），再整体通过激活函数（relu）后，既可以得到完整的输出。</li>
</ul>
</li>
</ul>
<p><img src="/pictures/image-20200703232550227.png" alt="image-20200703232550227"></p>
<ul>
<li>将上述进行形式化表述后可得下图：</li>
</ul>
<p><img src="/pictures/image-20200703232334195.png" alt="image-20200703232334195"></p>
<ul>
<li>那么该卷积层参数量的计算可以表述成下式：</li>
</ul>
<script type="math/tex; mode=display">
parameters^{[l]} = f^{[l]}*f^{[l]}*n_c^{[l-1]}*n_c^{[l]}+n_c^{[l]} \\\\
其中f^{[l]}是卷积核的尺寸大小 \\\\
n_c^{[l-1]}是该层输入特征图的通道数 \\\\
n_c^{[l]}是该层输出特征图的通道数 \\\\
式子中最后加上的 n_c^{[l]}表示输出的每个通道的偏置参数，有时可以忽略</script><h3 id="Pooling-layer"><a href="#Pooling-layer" class="headerlink" title="Pooling layer"></a>Pooling layer</h3><ul>
<li><p>池化层，也叫下采样层，常更在卷积层之后，形式上用于压缩特征图尺寸，同时可以突出卷积操作过后的重要特征。有一定的减少过拟合效果。</p>
</li>
<li><p>池化层进行的池化操作通常分为：最大池化（Max-Pooling）和平均池化（Average-Pooling）。</p>
</li>
<li><p>池化层的作用：</p>
<ul>
<li>为CNN增添了一定的不变性（invariance），包括：平移不变性、旋转不变性、尺度不变性；</li>
<li>保留特征图主要特征的同时还可以进行降维，一定程度的减少过拟合。</li>
<li>拿平移不变性来说：图象中的目标主体即便偏移了少许，但也不影响检测结果，因为通过卷积操作后会产生相似的特征。有点类似于对图像的 Resize操作后，主体特征仍在。</li>
</ul>
</li>
<li><p>池化层使用较多的是 Max pooling：</p>
<ul>
<li>max pooling作所做的其实是：如果在滤波器中任何地方检测到了突出的特征，就保留最大的数值；但是如果这个特征没有被检测到，如：可能图像左侧上方的四分之一区域就没有这个特征，那么那部分区域的数值的最大值仍然相当小。</li>
<li>具体操作如下图所示：</li>
</ul>
<p><img src="/pictures/cnnpooling.png" alt="cnnpooling.png"></p>
</li>
</ul>
<h3 id="Convnet的优点"><a href="#Convnet的优点" class="headerlink" title="Convnet的优点"></a>Convnet的优点</h3><ul>
<li>首先是参数共享：也就是同一卷积核可以应用于整张图片，相比于<strong>全连接</strong>，这能极大的减小参数量。</li>
<li>连接的稀疏性：特征图中的每个值，只对应前一个输入特征图的某一小部分值（也就是只依赖于这一小部分的值，不依赖于其它的值），建立了稀疏的连接。</li>
<li>一定的不变性，如：平移不变性；<ul>
<li>平移不变性：图象中的目标主体即便偏移了少许，但也不影响检测结果，因为通过卷积操作后会产生相似的特征。</li>
</ul>
</li>
</ul>
<p><img src="/pictures/image-20200704103937470.png" alt="image-20200704103937470"></p>
<h2 id="Yann-LeCun-Interview"><a href="#Yann-LeCun-Interview" class="headerlink" title="Yann LeCun Interview"></a>Yann LeCun Interview</h2><ul>
<li>如果你想进入AI领域， 就要让自己变得有用，比如：贡献自己的力量给开源社区，或者去实现一些网上找不到的标准算法，并把他们贡献出来让别人去使用；拿一篇你觉得很重要的文章，并去重新实现里面的算法，把他放到开源社区中去，或者去贡献某些开源项目；如果你写的东西很有趣，也有用，你就会被关注到，也许你会在一个你心仪的公司有一个好的工作，或者你会被你心仪的PhD项目录取，我觉得这是一个好的开始。</li>
<li>给开源社区做贡献是一个进入社区的好的方式， 把学到的知识回馈给别人。</li>
</ul>
<h2 id="经典的卷积网络"><a href="#经典的卷积网络" class="headerlink" title="经典的卷积网络"></a>经典的卷积网络</h2><h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><p><img src="/pictures/image-20200705145504908.png" alt="image-20200705145504908"></p>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p><img src="/pictures/image-20200705150520999.png" alt="image-20200705150520999"></p>
<h3 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG - 16"></a>VGG - 16</h3><ul>
<li>真正简化了神经网络结构 </li>
<li>结构很简洁，全篇使用的结构都是 CONV= 3 x 3 filter, stride = 1,same padding； MAX-POOL = 2 x 2, stride=2;</li>
<li>缺点也很明显：参数量过大有 138M 个。</li>
<li>VGG-16 与 VGG-19 性能差不多，更多的会使用 VGG-16。</li>
</ul>
<p><img src="/pictures/image-20200705151340754.png" alt="image-20200705151340754"></p>
<h2 id="Residual-Networks"><a href="#Residual-Networks" class="headerlink" title="Residual Networks"></a>Residual Networks</h2><h3 id="Residual-block"><a href="#Residual-block" class="headerlink" title="Residual block"></a>Residual block</h3><ul>
<li>残差连接，也叫 <code>short cut / skip connection</code>，即将神经网路某一段的<strong>输入</strong>跨过这一段距离直连到<strong>输出</strong>。<ul>
<li>通常实现形式有两种：The identity block and the convolutional block</li>
<li>若输入的特征图尺寸与输出的特征图尺寸一致，则可以直接将输入逐元素叠加到输出上。</li>
<li>若输入的特征图尺寸与输出的特征图尺寸<strong>不一致</strong>，则通常会通过 1x1的卷积操作改变特征图尺寸，然后再将输入逐元素叠加到输出上。</li>
</ul>
</li>
</ul>
<p><img src="/pictures/image-20200705160052849.png" alt="image-20200705160052849"></p>
<h3 id="为什么-ResNet-可以有效果？"><a href="#为什么-ResNet-可以有效果？" class="headerlink" title="为什么 ResNet 可以有效果？"></a>为什么 ResNet 可以有效果？</h3><ul>
<li>skip-connection 有助于解决梯度消失问题，这使得可以训练一个很深层的网络。</li>
<li>先从最差的情况谈起，Residual Block 最差的情况也只是一个恒等变换，将前面的什么都不变传递到后面。</li>
<li><p>额外增加的 Residual Block 学习起恒等函数比较简单，我们总能保证它不会影响总体的表现，甚至许多时候幸运的话可以提升网络的表现。</p>
</li>
<li><p>推导如下图所示：</p>
</li>
</ul>
<p><img src="/pictures/image-20200705162410718.png" alt="image-20200705162410718"></p>
<h3 id="1x1-卷积"><a href="#1x1-卷积" class="headerlink" title="1x1 卷积"></a>1x1 卷积</h3><ul>
<li>它本质上是一个全连接的神经网络，接收固定维数的输入，然后输出 filter 数量 维数的输出值；</li>
<li>用于缩减/增加维度；</li>
<li>若通道数目不变，增加 1 x 1卷积层，也可以增加非线性性。</li>
</ul>
<h2 id="Inception-Network"><a href="#Inception-Network" class="headerlink" title="Inception Network"></a>Inception Network</h2><ul>
<li><p>Inception 提出的动机：</p>
<ul>
<li>当为卷积层设计某一层时，需要选择不同的卷积核、池化层等；Inception 网络想表述的就是为什么不全用上。</li>
<li>当把 <code>1x1,3x3,5x5,max pooling</code> 全用上，将最终的结果拼接起来，然后让神经网络去学习它想要用到的参数，以及它想要用到的卷积核的大小。</li>
</ul>
</li>
<li><p>如下图所示：</p>
</li>
</ul>
<p><img src="/pictures/image-20200705174817616.png" alt="image-20200705174817616"></p>
<ul>
<li>当然这种设计也存在问题：即计算成本很大。<ul>
<li>降低计算成本的方法是，先使用 1 x 1的卷积核进行降维处理，如下述两图所示：</li>
</ul>
</li>
</ul>
<p><img src="/pictures/image-20200705175728639.png" alt="image-20200705175728639"></p>
<p><img src="/pictures/image-20200705175906241.png" alt="image-20200705175906241"></p>
<ul>
<li>Inception Network 主要由 Inception module所组成，基本的Inception module如下图所示：</li>
</ul>
<p><img src="/pictures/image-20200705180252143.png" alt="image-20200705180252143"></p>
<h2 id="使用-ConvNet-的建议"><a href="#使用-ConvNet-的建议" class="headerlink" title="使用 ConvNet 的建议"></a>使用 ConvNet 的建议</h2><ul>
<li>使用开源实现，先上Github找一找代码。</li>
<li><p>尽量进行迁移学习（Transfer Learnin）：</p>
<ul>
<li>数据量不多时，去掉最后的 <code>Softmax</code>分类层，改用新建立的分类层，并冻结前面各层的参数；可以将数据依次输入前面各层，并将输出向量保存下来，用于直接训练新建立的分类层，这样可以避免重复计算问题。</li>
<li>随着数据量的增大，就可以训练越来越多的层，直到可以使用<strong>预训练网络</strong>的权重作为初始权重，重新开始训练该网络</li>
</ul>
</li>
<li><p>进行一定的数据增强（Data Augmentation）:</p>
<ul>
<li>水平翻转（Mirroring）</li>
<li>随机裁剪（Random Cropping）</li>
<li>Rotation</li>
<li>Shearing</li>
<li>Local warping</li>
<li>……</li>
<li>Color shifting</li>
<li>PCA Color augmentation</li>
</ul>
</li>
<li><p>在训练中增加一定的随机扰动，并且采用并行实现，如下图所示：</p>
</li>
</ul>
<p><img src="/pictures/image-20200705185043775.png" alt="image-20200705185043775"></p>
<ul>
<li>Data VS hand-engineering 概览：</li>
</ul>
<p><img src="/pictures/image-20200705190431839.png" alt="image-20200705190431839"></p>
<ul>
<li>两条打比赛的建议：</li>
</ul>
<p><img src="/pictures/image-20200705190546428.png" alt="image-20200705190546428"></p>
<h1 id="Object-Detection（目标检测）"><a href="#Object-Detection（目标检测）" class="headerlink" title="Object Detection（目标检测）"></a>Object Detection（目标检测）</h1><ul>
<li>目标检测的引入：Object Localization 和 Landmark Detection</li>
<li>Object Localization：让神经网络输出一堆数值，用于表示图像中目标的<strong>位置和类别</strong>，这个想法很棒，如下图所示：<ul>
<li>在最终输出的向量中至少要包含2种类型的数据：目标类别和目标的边界框，如下图所示：</li>
</ul>
</li>
</ul>
<p><img src="/pictures/image-20200705203127470.png" alt="image-20200705203127470"></p>
<ul>
<li>简单的定义目标标签Y和损失函数：<ul>
<li>假设有四个类别，目标标签Y可定义为一个向量：[pc,bx,by,bh,bw,c1,c2,c3,c4];</li>
<li>其中 pc 指代图像中是否包含目标；bx,by,bh,bw，用来表示边界框；</li>
<li>c1,c2,c3,c4 可用于表示四个类别；</li>
<li>如下图所示：</li>
</ul>
</li>
</ul>
<p><img src="/pictures/image-20200705210421996.png" alt="image-20200705210421996"></p>
<ul>
<li>Landmark Detection：神经网络可以定位目标的边界框，但更普遍的情况可以让神经网络只输出重要的点，即图像中（X,Y）坐标，这个也被称作 Landmark。<ul>
<li>要注意对于 Landmark Detection：标签必须在不同的图片中保持一致性；对于 <code>Landmark</code> 的定义在不同的图片中必须保持一致。</li>
</ul>
</li>
</ul>
<p><img src="/pictures/image-20200705211422908.png" alt="image-20200705211422908"></p>
<h2 id="滑动窗口检测（Sliding-windows-detection）"><a href="#滑动窗口检测（Sliding-windows-detection）" class="headerlink" title="滑动窗口检测（Sliding windows detection）"></a>滑动窗口检测（Sliding windows detection）</h2><ul>
<li><p>实现方式：Convnet + sliding windows</p>
</li>
<li><p>训练：使用紧密裁剪的图像（只包含目标的图像）的数据集来训练卷积神经网络。</p>
</li>
<li>测试：给定多个size的矩形框，让这些矩形框在一定步长下滑过整个图像，然后判定每个方框内的区域是否含有目标。如下图示：</li>
</ul>
<p><img src="/pictures/image-20200706113150576.png" alt="image-20200706113150576"></p>
<ul>
<li><p>该种检测方式的缺点：</p>
<ul>
<li>巨大的计算成本：裁剪出了很多不同的矩形框，每个矩形框都要通过Convnet的计算。单次Convnet的计算开销就比较大。</li>
<li>颗粒度上：除非使用很小的颗粒度，也就是很小的步长，这样才能比较精确的定位到图像中的目标；但这会造成巨大的计算成本。</li>
</ul>
</li>
<li><p>为了减少计算量，提出了滑动窗口的卷积化实现：</p>
<ul>
<li>滑动窗口中，重叠部分的图像，在卷积网络中的计算是重复的，因此可以想方法共享这部分的计算结果，这就是下图的设计，最终输出的结果，可以对应到原始图片不同区域（滑动窗口经过区域）的卷积结果；</li>
<li>不在裁剪后的输入图像上分别进行前向传播，而是合并成一个前向传播。</li>
<li>如下两图所示：</li>
</ul>
</li>
</ul>
<p><img src="/pictures/image-20200706114658214.png" alt="image-20200706114658214"></p>
<p><img src="/pictures/image-20200706121231188.png" alt="image-20200706121231188"></p>
<ul>
<li>当然上述滑动窗口的卷积化实现的缺点也是很明显的：<ul>
<li>窗口边框的定位并不精确；</li>
<li>如何很好的打标签也是个问题。</li>
</ul>
</li>
</ul>
<h2 id="边界框预测（Bounding-Box-Predictions）"><a href="#边界框预测（Bounding-Box-Predictions）" class="headerlink" title="边界框预测（Bounding Box Predictions）"></a>边界框预测（Bounding Box Predictions）</h2><ul>
<li>为了解决卷积化的滑动窗口预测边界框不准确的问题，提出了Yolo系列算法，为了更精确的预测边界框；</li>
<li>Yolo 算法：You only look once，大概实现思想如下（此时还没有包括 Anchor的思想，后续会介绍）：<ul>
<li>将整体图片分成 3x3的网格（这是示例，实际中可以更精细比如：19x19）；</li>
<li>每个网格负责输出一个向量 Y=[Pc,bx,by,bw,bh,c1,c2,c3]，用于做最后的预测（与Object Localization很像）；<ul>
<li>其中Pc指代该网格是否包含目标物；</li>
<li>bx,by,bw,bh 表示目标的位置</li>
<li>c1,c2,c3 表示用于分类的类别</li>
</ul>
</li>
<li>根据网格数和最终单个网格要输出的向量维度，以此来设计全卷积网络（也就是通过一系列的卷积层、池化层等将图像的维度逐渐转化成 3x3x8—示例的最终输出维度），就是Yolo了。</li>
<li>该算法的优势在于：<ul>
<li>该神经网络可以精确地输出目标的边界框（任意长宽比，不会受限于滑动窗口的步长） ；</li>
<li>算法通过卷积的形式实现-共享了部分计算量，不需要将网格逐次输入；</li>
<li>运算速度较快，基本可以实时检测。</li>
</ul>
</li>
<li>该算法的劣势在于：<ul>
<li>每个网格中不能超过一个目标物。</li>
</ul>
</li>
<li>需要注意的是<strong>将目标物分配到网格中的方式</strong>：<ul>
<li>先找到目标物的中心点，再根据中心点的位置将它分配到包含该中心点的网格中；</li>
<li>所以对于每个目标物，即便跨越了多个网格，也只会分配到一个网格。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="IOU、NMS-和-Anchor-Box"><a href="#IOU、NMS-和-Anchor-Box" class="headerlink" title="IOU、NMS 和 Anchor Box"></a>IOU、NMS 和 Anchor Box</h2><ul>
<li>IOU：Intersection Over Union，又称为<strong>交并比</strong>，用于衡量两个方框的重叠程度<ul>
<li>IOU=两个方框的交集/两个方框的并集；</li>
<li>交并比(Intersection Over Union)，它既可以用来评价你的目标检测算法，也可以用于，往目标检测算法中加入其他特征部分，来进一步改善它 。</li>
<li>如下图所示：</li>
</ul>
</li>
</ul>
<p><img src="/pictures/image-20200706135247615.png" alt="image-20200706135247615"></p>
<ul>
<li>NMS：Non-max Suppression，又称<strong>非极大值抑制</strong>。检测算法可能会对同一目标有多次检测：非极大值抑制是保证算法对每个目标只得到一个检测结果。<ul>
<li>非极大值抑制思想：输出有着最大可能性的分类判断结果，而抑制那些与该结果有交集（邻近），但非最大可能性的方框。</li>
</ul>
</li>
<li><p>NMS算法细节：假设针对每个网格，都输出了一个五维向量[Pc,bx.by.bh,bw]，其中Pc表示网格中包含目标的概率，其余是边界框的描述。</p>
<ol>
<li>丢弃所有低概率输出框（如：Pc&lt;0.6的）</li>
<li>对剩余没有被丢弃的输出：重复选出有着最大概率（Pc值最大）作为一个 Prediction；将与Prediction有重叠且IOU&gt;0.5的输出框也抛弃掉；直到所有的输出框都被处理过。</li>
<li>最终每个输出框不是判定为 Prediction（预测结果），就是被抛弃。</li>
</ol>
</li>
<li><p>如下图所示：</p>
</li>
</ul>
<p><img src="/pictures/image-20200706142443842.png" alt="image-20200706142443842"></p>
<ul>
<li>Anchor Box：表现形式为提前给定可能目标的<strong>长宽比</strong>，然后在预测输出中增加对该长宽比的预测。<ul>
<li>Anchor的引入：Yolo的每个网格只能检测一个目标，要是想一个网格要检测多个目标，应该怎么办？</li>
<li>因此引入Anchor box，也就是同一网格可以根据给定长宽比对目标进行预测，这样一个网格就可以预测多个目标了，如下图示：</li>
</ul>
</li>
</ul>
<p><img src="/pictures/image-20200706152516446.png" alt="image-20200706152516446"></p>
<ul>
<li>从预测输出上：拓宽了原来 预测标签 Y的维度，根据不同的 Anchor box来进行预测；此时有两个 不同形状的 Anchor box之后，Y 会被拓展成[Pc,bx.by.bh,bw,c1,c2,c3,Pc,bx.by.bh,bw,c1,c2,c3].</li>
<li>对于Anchor box 形状的选择上（长宽比的选择），可以使用聚类分析（K-means）在原始数据集上的长宽比类别中心来进行设置。</li>
</ul>
<ul>
<li><p>在有了Anchor box的概念后，Yolo 算法可以进行如下表示：</p>
<ol>
<li>组织训练集：假设总共三个类别（不用为背景设置额外类别）；两个Anchor box</li>
</ol>
<p><img src="/pictures/image-20200706153241763.png" alt="image-20200706153241763"></p>
<ol>
<li>对预测结果进行处理</li>
</ol>
<p><img src="/pictures/image-20200706153851813.png" alt="image-20200706153851813"></p>
</li>
</ul>
<h2 id="Region-Proposals"><a href="#Region-Proposals" class="headerlink" title="Region Proposals"></a>Region Proposals</h2><ul>
<li>简单解释 Region Proposals：想较于滑动窗口，或者整张图片进行的卷积操作 ，Region Proposals  尝试选取少部分有意义的区域（也就是少量的窗口）用于进行卷积操作。</li>
<li>实现 Region Proposals 的方式上，主要是想通过 segmentation algorithm 来实现。</li>
</ul>
<p><img src="/pictures/image-20200706154904663.png" alt="image-20200706154904663"></p>
<h1 id="Face-Recognition"><a href="#Face-Recognition" class="headerlink" title="Face Recognition"></a>Face Recognition</h1><ul>
<li>通用的人脸识别系统分为：<strong>人脸识别</strong>和<strong>活体检测</strong>两部分；两者均可通过CNN来实现，只是这是两个不同的任务。</li>
<li><strong>人脸识别</strong>与<strong>人脸验证</strong>是有区别的，要注意区分：<ul>
<li>人脸识别解决的是困难的 1对多的匹配问题，即：首先拥有包含多个人脸照的数据库；然后输入一张图片，输出数据库中匹配上的人脸照片ID或者未识别照片。</li>
<li>人脸验证解决的是简单的 1对1的匹配问题，即：输入图片。姓名/ID；然后输出判断，这张照片是不是对应于这个人。</li>
<li>如下图所示：</li>
</ul>
</li>
</ul>
<p><img src="/pictures/image-20200706200918442.png" alt="image-20200706200918442"></p>
<ul>
<li>对于人脸识别和人脸验证：<ul>
<li><code>triplet loss</code>是一个有效的损失函数用于训练一个神经网络来学习一张人脸图片的编码特征（某个向量表示）。</li>
<li>对于图片进行同样的特征提取后生成的编码或者向量表示，可以适用于人脸识别和人脸验证。</li>
<li>可以通过衡量两个不同向量表示之间的距离来判断这两个向量是否表示的是同一个人脸。</li>
</ul>
</li>
</ul>
<h2 id="单样本学习（One-Shot-Learning）"><a href="#单样本学习（One-Shot-Learning）" class="headerlink" title="单样本学习（One Shot Learning）"></a>单样本学习（One Shot Learning）</h2><ul>
<li>单样本学习问题：必须在只见过一张照片（只见过一个人的脸部-一个样本）的前提下，认出一个人。</li>
<li>实质也就是要学习一个<strong>相似性函数</strong>，如下图示：</li>
</ul>
<p><img src="/pictures/image-20200706202152963.png" alt="image-20200706202152963"></p>
<h2 id="Siamese-network"><a href="#Siamese-network" class="headerlink" title="Siamese network"></a>Siamese network</h2><ul>
<li>该网络的学习目的：用于解决单样本学习问题，是想让相似照片的特征向量表示之间的距离更小（比如欧氏距离），不同相片的特征向量表示之间的距离更大。</li>
<li>该网络主要思想：<ul>
<li>接受两个图像作为输入，如下图所示，图中上下两个模型，都由CNN构成，两个模型的参数值完全相同。不同于传统CNN的地方，是Siamese网络并不直接输出类别，而是输出一个向量(比如128维的特征向量)。</li>
<li>若输入的图像X1和X2为同一个人，则上下两个模型输出的一维向量欧氏距离较小；若输入的图像X1和X2不是同一个人，则上下两个模型输出的一维向量欧氏距离较大。</li>
<li>如下述两幅图片所示：</li>
</ul>
</li>
</ul>
<p><img src="/pictures/image-20200706203216698.png" alt="image-20200706203216698"></p>
<p><img src="/pictures/image-20200706203255638.png" alt="image-20200706203255638"></p>
<ul>
<li>训练Siamese 网络：<ul>
<li>首先是损失函数的设计，然后使用特定的梯度下降法寻找最优解。</li>
<li>一般损失函数定义如下所示：<ul>
<li>即给定 Anchor、Positive、Negative；Anchor可定义为锚点图片，Positive为正样本图片与Anchor表示的是同一个人，Negative是负样本与Anchor表示的不是同一个人；</li>
<li>Anchor、Positive的距离 d(A,P)，Anchor、Negative的距离d(A,N)，此时的优化目标就是想d(A,P)能远小于d(A,N)。</li>
</ul>
</li>
<li>组织训练集时就是组织三元组（Anchor、Positive、Negative），公式中的参数α，是一个超参数，用于做Positive、Negative的最小边界（margin），能避免模型输出的都是零向量。</li>
</ul>
</li>
</ul>
<p><img src="/pictures/cnnsiameseloss.png" alt="cnnsiameseloss.png"></p>
<h2 id="Triplet-Loss"><a href="#Triplet-Loss" class="headerlink" title="Triplet Loss"></a>Triplet Loss</h2><ul>
<li>参考论文：Schroff et al. 2015,FaceNet: A unified embedding for face recognition and clustering </li>
<li>相比于 Siamese的同时输入两张图片，Triplet更进一步同时输入三张图片，分别作为：Anchor、Positive、Negative；</li>
<li>训练目标是想通过ConvNet的变化后，对由图片所得到的向量（以F(A),F(P),F(N)分别表示）进行距离度量时，F(A),F(P)的距离可以更近，F(A),F(N)的距离可以更远。如下图所示：</li>
</ul>
<p><img src="/pictures/image-20200706205851202.png" alt="image-20200706205851202"></p>
<ul>
<li>Triplet Loss的定义如下：</li>
</ul>
<script type="math/tex; mode=display">
L(A,P,N)=max(||F(A)-F(P)||^2-||F(A)-F(N)||^2+\alpha,0) \\\\
J=\sum_{i=1}^{m}L(A^{(i)},P^{(i)},N^{(i)})</script><ul>
<li>Triplet 网络的重点是训练集的组织，即对输入网络的三元组如何进行选取。<ul>
<li>既不能随机选取，这样会导致条件 :d(A,P)+alpha &lt;= d(A,N) 过于容易满足；</li>
<li>要选择那些比较“hard”去训练（也就是难以区分的）的三元组。</li>
<li>三元组的选择，可以参考开头的论文：</li>
</ul>
</li>
</ul>
<p><img src="/pictures/image-20200706210610393.png" alt="image-20200706210610393"></p>
<h2 id="Face-Verification-and-Binary-Classification"><a href="#Face-Verification-and-Binary-Classification" class="headerlink" title="Face Verification and Binary Classification"></a>Face Verification and Binary Classification</h2><ul>
<li>对于人脸验证问题，也可以不使用 Siamese/Triplet 来进行相似度衡量，可以直接将其当作一个二分类问题：<ul>
<li>利用ConvNet的输出，后续接一个非线性的分类器（线性计算+非线性变换）用于进行二分类预测，也是可以的，如下图所示：</li>
</ul>
</li>
</ul>
<p><img src="/pictures/image-20200706212002904.png" alt="image-20200706212002904"></p>
<h1 id="Neural-Style-Transfer"><a href="#Neural-Style-Transfer" class="headerlink" title="Neural Style Transfer"></a>Neural Style Transfer</h1><h2 id="CNN网络与神经风格迁移"><a href="#CNN网络与神经风格迁移" class="headerlink" title="CNN网络与神经风格迁移"></a>CNN网络与神经风格迁移</h2><ul>
<li>神经风格迁移（Neural Style Transfer）是对已经<strong>训练好的CNN</strong>的一个重要应用，也是对CNN到底学习到了什么的一个很好的解答。神经风格迁移（Neural Style Transfer）有时候也叫图片风格迁移。</li>
<li>Neural Style Transfer的过程如下图所示：</li>
</ul>
<p><img src="/pictures/image-20200706212455827.png" alt="image-20200706212455827"></p>
<ul>
<li>为了实现Neural Style Transfer, 我们需要查看卷积神经网络在不同层中提取的特征值, 包括浅层和深层的特征值。也就是深度卷积网络到底学习到了什么东西？<ul>
<li>在浅层提取的多数是纹理、颜色等低级别的形状特征；</li>
<li>随着网络层的加深，低级别特征之间相互组合会逐渐提取出高级别的语义特征，空间特征等，也就是网络开始对图像进行抽象识别，判断图像是雨伞、花或者其它物体等；</li>
<li>对于卷积神经网络经过了激活函数后的<strong>深层网络层</strong>所表示的信息，我们可以将其理解成这是图像所存在的深层风格；</li>
<li>对于<strong>浅层网络层</strong>所表示的信息，可以将其理解成图像所拥有的浅层风格（或者理解成图像包含的浅层内容，有多少纹理、什么颜色等等）。</li>
<li>当我们将一个图像的内容和风格可以大致分开后，就可以将该图像的深层风格逐步迁移到另一张图像的内容上了。</li>
</ul>
</li>
<li>参考论文：Zeiler and Fergus.,2013,Visualizing and understanding convolutional networks</li>
</ul>
<p><img src="/pictures/image-20200706214025099.png" alt="image-20200706214025099"></p>
<h2 id="Gram-Matrix"><a href="#Gram-Matrix" class="headerlink" title="Gram Matrix"></a>Gram Matrix</h2><ul>
<li><p>在理解了CNN不同层之间的抽象表示之后，我们还缺少一样工具，也就是如何度量两个图像风格的差异，这里需要引入<strong>Gram矩阵</strong>，用于度量两个图像风格的差异：</p>
<ul>
<li>Gram矩阵的定义如下所示：</li>
</ul>
<p><img src="/pictures/cnngrammatrix.png" alt="cnngrammatrix"></p>
<ul>
<li>在使用Gram矩阵时，可以将CNN某一层输出的特征图拼接成一个大的 NxM的矩阵，也可以分别计算不同通道的特征图，也就是逐个输入两个图片在不同通道上的特征图；</li>
<li>通过将两个不同图片的在CNN某一层的特征图输入Gram矩阵，就可以较好的度量每个特征之间的<strong>相关性</strong>。</li>
<li>Gram矩阵，实际上可看做是特征图之间的偏心协方差矩阵（即没有减去均值的协方差矩阵）<ul>
<li>在特征图中，每一个数字都来自于一个特定滤波器在特定位置的卷积，因此每个数字就代表一个特征的强度，而Gram计算的实际上是两两特征之间的<strong>相关性</strong>；</li>
<li>对于Gram矩阵的非对角线元素，当同一通道的特征图上不同的特征值相乘时：原来小的就会变的更小，原来大的就变的更大；这就表明了二个不同特征值的关系：哪两个特征是同时出现的，哪两个是此消彼长的等等；</li>
<li>对于Gram矩阵的对角线元素，还体现了每个特征在图像中出现的量（强度），因此，Gram矩阵有助于把握整个图像的大体风格。</li>
<li>有了表示风格的Gram矩阵后，要度量两个图像风格的差异，只需比较他们Gram 矩阵的差异即可，这就将一个抽象的问题具体化了。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="神经风格迁移的实现"><a href="#神经风格迁移的实现" class="headerlink" title="神经风格迁移的实现"></a>神经风格迁移的实现</h2><ul>
<li>实现神经风格迁移的大致过程如下：<ul>
<li>首先我们采用的是一个已经训练好的CNN网络，如：VGG-16。</li>
<li>我们将风格图片（Style image）和内容图片（Content image）的输入CNN网络，并将输出特征提取出来存储。</li>
<li>风格图片经过下图左边的网络，我们利用<strong>Gram矩阵</strong>计算每个层上的<strong>风格表征</strong>并存储。</li>
<li>内容图片经过下图右边的这个网络，我们计算某一层的<strong>内容表征</strong>（较高层）并存储。</li>
<li>然后再向网络中输入一张随机的白噪音图片，如下图示经过了中间这个网络，计算该图片在网络每一层的风格表征和特定层的内容表征。</li>
<li>通过将上述的风格表征、内容表征输入损失函数，分别计算每一部分的损失，通过线性加权后组合成最终的损失值。</li>
<li>损失值通过下图中间的神经网络进行反向传播，每次传播到输入端后，用此来更新白噪音图片的像素值，据此使得它同时尽可能的去匹配内容图的内容以及风格图的风格。</li>
<li>当损失值降低到一定程度后，输出不断更新的图片，即为最终的神经风格迁移图片。</li>
</ul>
</li>
</ul>
<p><img src="/pictures/cnnstyletransfer.png" alt="cnnstyletransfer"></p>
<ul>
<li><p>神经风格迁移的详细表述如下：</p>
<ul>
<li>参考论文：<ul>
<li>A neural algorithm of artistic style. Images on slide generated by Justin Johnson</li>
<li>A neural algorithm of artistic style</li>
</ul>
</li>
<li>总体损失函数定义为：</li>
</ul>
<script type="math/tex; mode=display">
J(G)=\alpha J_{content}(C,G) + \beta J_{style}(S,G) \\\\
\alpha,\beta 是超参数</script><ul>
<li><p>其中C-Content，内容图片；G-Generate，生成图片；S-Style，风格图片。</p>
</li>
<li><p>优化目标在于找到好的生成图片，如下图所示：</p>
</li>
</ul>
<p><img src="/pictures/image-20200706214830502.png" alt="image-20200706214830502"></p>
</li>
<li><p>其中 Content 损失函数的定义如下所示：</p>
</li>
</ul>
<script type="math/tex; mode=display">
J_{content}(C,G)=\frac{1}{2}||a^{[l](c)}-a^{[l](G)}||^2,element-wise级别的计算 \\\\
其中a^{[l](c)}、a^{[l](G)}分别代表内容图片和生成图片在CNN第l层输出的特征值</script><p><img src="/pictures/image-20200706215315522.png" alt="image-20200706215315522"></p>
<ul>
<li>其中 Style 损失函数的定义如下所示：<ul>
<li>针对CNN第l层风格损失的计算，对于多层风格损失，进行线性组合即可。</li>
</ul>
</li>
</ul>
<p><img src="/pictures/image-20200706221906851.png" alt="image-20200706221906851"></p>
<ul>
<li>Style损失函数的计算如下所示：使用了Gram矩阵</li>
</ul>
<p><img src="/pictures/image-20200706221800278.png" alt="image-20200706221800278"></p>
<h1 id="1D-and-3D-Generalizations-推广到1维和3维"><a href="#1D-and-3D-Generalizations-推广到1维和3维" class="headerlink" title="1D and 3D Generalizations(推广到1维和3维)"></a>1D and 3D Generalizations(推广到1维和3维)</h1><ul>
<li>推广过程的实质也就是<strong>卷积核维度</strong>的变化，如下图所示：</li>
</ul>
<p><img src="/pictures/image-20200706223240303.png" alt="image-20200706223240303"></p>

    </div>

    
    
    
        <div class="reward-container">
  <div>觉着不错，打赏一下吧。</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/pictures/wechatpay.jpg" alt="刘涛 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/pictures/alipay.jpg" alt="刘涛 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>刘涛
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://lt4hyl.top/2020-08-28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%884%EF%BC%89.html" title="深度学习笔记（4）">http://lt4hyl.top/2020-08-28/深度学习笔记（4）.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 基础知识，深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020-08-27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%883%EF%BC%89.html" rel="prev" title="深度学习笔记（3）">
      <i class="fa fa-chevron-left"></i> 深度学习笔记（3）
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Computer-Vision%EF%BC%88%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%EF%BC%89"><span class="nav-number">1.</span> <span class="nav-text">Computer Vision（计算机视觉）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Convolutional-Neural-Networks"><span class="nav-number">1.1.</span> <span class="nav-text">Convolutional Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%E3%80%81padding%E3%80%81stride"><span class="nav-number">1.1.1.</span> <span class="nav-text">卷积操作、padding、stride</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97"><span class="nav-number">1.1.2.</span> <span class="nav-text">卷积层参数量计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pooling-layer"><span class="nav-number">1.1.3.</span> <span class="nav-text">Pooling layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Convnet%E7%9A%84%E4%BC%98%E7%82%B9"><span class="nav-number">1.1.4.</span> <span class="nav-text">Convnet的优点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Yann-LeCun-Interview"><span class="nav-number">1.2.</span> <span class="nav-text">Yann LeCun Interview</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%8F%E5%85%B8%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C"><span class="nav-number">1.3.</span> <span class="nav-text">经典的卷积网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LeNet-5"><span class="nav-number">1.3.1.</span> <span class="nav-text">LeNet-5</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AlexNet"><span class="nav-number">1.3.2.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VGG-16"><span class="nav-number">1.3.3.</span> <span class="nav-text">VGG - 16</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Residual-Networks"><span class="nav-number">1.4.</span> <span class="nav-text">Residual Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Residual-block"><span class="nav-number">1.4.1.</span> <span class="nav-text">Residual block</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88-ResNet-%E5%8F%AF%E4%BB%A5%E6%9C%89%E6%95%88%E6%9E%9C%EF%BC%9F"><span class="nav-number">1.4.2.</span> <span class="nav-text">为什么 ResNet 可以有效果？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1x1-%E5%8D%B7%E7%A7%AF"><span class="nav-number">1.4.3.</span> <span class="nav-text">1x1 卷积</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inception-Network"><span class="nav-number">1.5.</span> <span class="nav-text">Inception Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-ConvNet-%E7%9A%84%E5%BB%BA%E8%AE%AE"><span class="nav-number">1.6.</span> <span class="nav-text">使用 ConvNet 的建议</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Object-Detection%EF%BC%88%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%89"><span class="nav-number">2.</span> <span class="nav-text">Object Detection（目标检测）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%A3%80%E6%B5%8B%EF%BC%88Sliding-windows-detection%EF%BC%89"><span class="nav-number">2.1.</span> <span class="nav-text">滑动窗口检测（Sliding windows detection）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BE%B9%E7%95%8C%E6%A1%86%E9%A2%84%E6%B5%8B%EF%BC%88Bounding-Box-Predictions%EF%BC%89"><span class="nav-number">2.2.</span> <span class="nav-text">边界框预测（Bounding Box Predictions）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IOU%E3%80%81NMS-%E5%92%8C-Anchor-Box"><span class="nav-number">2.3.</span> <span class="nav-text">IOU、NMS 和 Anchor Box</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Region-Proposals"><span class="nav-number">2.4.</span> <span class="nav-text">Region Proposals</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Face-Recognition"><span class="nav-number">3.</span> <span class="nav-text">Face Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%EF%BC%88One-Shot-Learning%EF%BC%89"><span class="nav-number">3.1.</span> <span class="nav-text">单样本学习（One Shot Learning）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Siamese-network"><span class="nav-number">3.2.</span> <span class="nav-text">Siamese network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Triplet-Loss"><span class="nav-number">3.3.</span> <span class="nav-text">Triplet Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Face-Verification-and-Binary-Classification"><span class="nav-number">3.4.</span> <span class="nav-text">Face Verification and Binary Classification</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Neural-Style-Transfer"><span class="nav-number">4.</span> <span class="nav-text">Neural Style Transfer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN%E7%BD%91%E7%BB%9C%E4%B8%8E%E7%A5%9E%E7%BB%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB"><span class="nav-number">4.1.</span> <span class="nav-text">CNN网络与神经风格迁移</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gram-Matrix"><span class="nav-number">4.2.</span> <span class="nav-text">Gram Matrix</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="nav-number">4.3.</span> <span class="nav-text">神经风格迁移的实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1D-and-3D-Generalizations-%E6%8E%A8%E5%B9%BF%E5%88%B01%E7%BB%B4%E5%92%8C3%E7%BB%B4"><span class="nav-number">5.</span> <span class="nav-text">1D and 3D Generalizations(推广到1维和3维)</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">刘涛</p>
  <div class="site-description" itemprop="description">为人民日益增长的美好生活需要而读书：机器学习、深度学习、c++、python、C#</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">30</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">京ICP备2020034167号-1 </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">刘涛</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">147k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">2:14</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;
      
      
      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          //console.log(error);
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"MxrUXwts7rlzq762NfLCiSVk-gzGzoHsz","app_key":"bqlnUht41VcOv8c5hK8DrOyz","server_url":"https://mxruxwts.lc-cn-n1-shared.com","security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@2.7.8/unpacked/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
