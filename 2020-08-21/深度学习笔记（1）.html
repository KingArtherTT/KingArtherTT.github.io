<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.0.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lt4hyl.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Neural Networks and Deep Learning 深度学习笔记的系列博客属于对 Andrew Ng的深度学习专项课程的回顾记录。该套课程可以于Coursera上获得，当然B站也早有同学搬运过去了。  “Scale”的引入 问题引入：如果有人问你传统机器学习与深度神经网络的区别，那么请看下图：    在小数据集时，可能手工设计的组件能取得更好的效果，但上图所示的是在更大数据集上的表">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习笔记（1）">
<meta property="og:url" content="http://lt4hyl.top/2020-08-21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89.html">
<meta property="og:site_name" content="Lancelot的小站">
<meta property="og:description" content="Neural Networks and Deep Learning 深度学习笔记的系列博客属于对 Andrew Ng的深度学习专项课程的回顾记录。该套课程可以于Coursera上获得，当然B站也早有同学搬运过去了。  “Scale”的引入 问题引入：如果有人问你传统机器学习与深度神经网络的区别，那么请看下图：    在小数据集时，可能手工设计的组件能取得更好的效果，但上图所示的是在更大数据集上的表">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200518150357203.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200624104855632.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200621170224254.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200629095055434.png">
<meta property="og:image" content="http://lt4hyl.top/pictures/image-20200629103244101.png">
<meta property="article:published_time" content="2020-08-21T08:06:42.000Z">
<meta property="article:modified_time" content="2020-08-21T10:06:14.859Z">
<meta property="article:author" content="刘涛">
<meta property="article:tag" content="基础知识，深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://lt4hyl.top/pictures/image-20200518150357203.png">

<link rel="canonical" href="http://lt4hyl.top/2020-08-21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>深度学习笔记（1） | Lancelot的小站</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?43b02ed86a701dce319ff86325707452";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Lancelot的小站</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">记录点滴成长：AI所向，吾之所往</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://lt4hyl.top/2020-08-21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="刘涛">
      <meta itemprop="description" content="为人民日益增长的美好生活需要而读书：机器学习、深度学习、c++、python、C#">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lancelot的小站">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习笔记（1）
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-08-21 16:06:42 / 修改时间：18:06:14" itemprop="dateCreated datePublished" datetime="2020-08-21T16:06:42+08:00">2020-08-21</time>
            </span>

          
            <span id="/2020-08-21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89.html" class="post-meta-item leancloud_visitors" data-flag-title="深度学习笔记（1）" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Neural-Networks-and-Deep-Learning"><a href="#Neural-Networks-and-Deep-Learning" class="headerlink" title="Neural Networks and Deep Learning"></a>Neural Networks and Deep Learning</h1><ul>
<li>深度学习笔记的系列博客属于对 Andrew Ng的深度学习专项课程的回顾记录。该套课程可以于Coursera上获得，当然B站也早有同学搬运过去了。</li>
</ul>
<h2 id="“Scale”的引入"><a href="#“Scale”的引入" class="headerlink" title="“Scale”的引入"></a>“Scale”的引入</h2><ul>
<li>问题引入：如果有人问你传统机器学习与深度神经网络的区别，那么请看下图：</li>
</ul>
<p><img src="/pictures/image-20200518150357203.png" alt="image-20200518150357203"></p>
<ul>
<li>在小数据集时，可能手工设计的组件能取得更好的效果，但上图所示的是在更大数据集上的表现。随着网络规模的增加和数据量的增加，性能会表现的越来越优越（当然训练也越来越困难）。</li>
<li>“Scale”一直在推动着神经网络的发展，这个“Scale”不仅指神经网络的规模，还有数据的规模。</li>
</ul>
<a id="more"></a>
<h2 id="Geffrey-Hinton-专访"><a href="#Geffrey-Hinton-专访" class="headerlink" title="Geffrey Hinton 专访"></a>Geffrey Hinton 专访</h2><ul>
<li>深度学习专项课程总共分为五门课程，每门课程的最后Andrew Ng都邀请到了一些学界大牛来做分享，第一门课第一周邀请到的是 <strong>Geffrey Hinton</strong>。</li>
<li>访问视频中印象最深的是下述两点：<ol>
<li>关于科研上：读足够多的文献，直到产生自己的直觉，然后相信自己的直觉，并坚持做下去。</li>
<li>对学生的建议：对于读研究生的学生，尝试找一个与你有着相似想法的导师，因为如果你的工作得到导师的认可，你会从导师那里得到很多好的建议，他们会花时间在你身上。如果你的工作导师不认可，也会得到一些建议，但并不会有多大帮助。</li>
</ol>
</li>
</ul>
<h2 id="回归、分类与逻辑回归"><a href="#回归、分类与逻辑回归" class="headerlink" title="回归、分类与逻辑回归"></a>回归、分类与逻辑回归</h2><ul>
<li>首先回归、分类与逻辑回归 都包含同样的一个过程，即组织数据，前向传播，反向传播，训练网络和最终的获取预测等。</li>
<li>对于上述三者首先要进行区分的是<strong>回归</strong>与<strong>分类</strong>：这主要看两者的输出<ul>
<li>回归通常给出的预测值是连续型的数值，如：0-1之间的任意数值。</li>
<li>分类通常输出的是离散的类别，如：0，1，2，3表示某一固定类别。</li>
</ul>
</li>
<li><p>区分过回归与分类后，就可以很明确<strong>逻辑回归</strong>是什么了，首先它属于<strong>回归</strong>，输出的是连续型数值，但很容易让人混淆的是，通常我们会说逻辑回归是可以用来解决二分类问题的机器学习方法，即输出（0或1）。</p>
<ul>
<li>这看起来就矛盾了，但理解这两种不同说法的关键在于：理解<strong>分类</strong>和<strong>回归</strong>本质是一样，分类是将回归的输出离散化。</li>
<li>逻辑回归模型的输出通常在最后一层通过 Sigmoid函数，将连续型的数值映射到（0，1）之间，我们可以将0.5作为阈值，若输出值大于0.5分类为1；若输出值小于0.5则分类为0。此时就可以解释上述矛盾了。</li>
<li>最后值得注意的是，在通常的分类问题中，模型A输出0.1，模型B输出0.49，若只简单的进行分类评判，这通常很难评价模型A，B的好坏，此时就要看实际回归值上的差异了。</li>
</ul>
</li>
<li><p>逻辑回归（Logistic Regression）与线性回归（Linear Regression）：</p>
<ul>
<li>这两者都是一种广义线性模型，逻辑回归假设因变量 y 服从伯努利分布，而线性回归假设因变量 y 服从高斯分布。</li>
<li>当逻辑回归除去最后的非线性变换函数后，基本就跟线性回归一摸一样了。</li>
</ul>
</li>
</ul>
<h2 id="Pieter-Abbeel-专访"><a href="#Pieter-Abbeel-专访" class="headerlink" title="Pieter Abbeel 专访"></a>Pieter Abbeel 专访</h2><ul>
<li>深度强化学习的下一步是什么？<ul>
<li>进行行为模仿，为人类提供建议，如：客服问题推荐。</li>
<li>行为模仿：先用监督学习，来模仿人类做的无论什么事情，之后渐渐地，用强化学习来让机器从更长时间的视野上去思考。</li>
</ul>
</li>
<li>强化学习也存在些许问题，也就是现在每次的功能学习都是从头开始，没有办法很好的利用以前学习到的知识。</li>
<li>如果某人想进入机器学习或深度学习领域，他们应当申请一个博士项目还是应该去大公司找一个工作呢?<ul>
<li>我觉得很大一部分应该取决于你能得到多少的指导，在博士项目中，你的指导能得到有力保证。因为教授，也就是你的导师，他的职责就是密切照看你的研究 ，做一切他们能做的事情，去从某种意义上塑造你，助你在想做的任何事情上变得更强。</li>
<li>不过这不意味着在公司就不可能学习，因为有很多公司有很好的职业导师，他们热爱的事情是教育人、加强人等等，区别在于：相比博士项目，这种指导的保障并不是那么强 ，因为博士项目的关键之处就是，你是想学习的并且有一个人帮助你学习。</li>
</ul>
</li>
</ul>
<h2 id="激活函数-Activation-Functions"><a href="#激活函数-Activation-Functions" class="headerlink" title="激活函数(Activation Functions)"></a>激活函数(Activation Functions)</h2><script type="math/tex; mode=display">
sigmoid(z) = \frac{1}{1+e^{-x}} \\\\
tanh(z)=\frac{e^x+e^{-x}}{e^x-e^{-x}} \\\\
Relu(z) =max(0,z) \\\\
leakyRelu(z) =max(0.01*z,z)</script><ul>
<li>如下图所示：</li>
</ul>
<p><img src="/pictures/image-20200624104855632.png" alt="image-20200624104855632"></p>
<ul>
<li>几点优劣评判：<ul>
<li><code>tanh</code> 总是比 <code>sigmoid</code> 更优的选择；<code>tanh</code> 有居中效果，值域为（-1，1），均值更接近于0；</li>
<li><code>sigmoid</code> 和 <code>tanh</code> 的共同缺点：<code>z</code>过大过小时，梯度会很小，面临梯度消失问题。</li>
<li>若使用的是二元分类，即输出为0，1，那么 <code>sigmoid</code> 对于输出层是一个非常自然的选择。但其他情况尽量不使用。</li>
<li>大部分情况下，可以默认使用<code>Relu</code>函数；<code>Relu</code>的一个缺点是当 <code>z&lt;0</code> 时导数为0；因此有改进版本 <code>leaky Relu</code>。</li>
</ul>
</li>
</ul>
<h2 id="初始化参数问题"><a href="#初始化参数问题" class="headerlink" title="初始化参数问题"></a>初始化参数问题</h2><ul>
<li><p>初始化神经网络的权重参数时，很自然的会想到能否将权重矩阵全部初始化为0，但这是不行的。</p>
<ul>
<li>若将权重矩阵全部初始化为0，会导致权重矩阵的“对称失效问题”，无法打破权重矩阵的对称性，导致最终：隐藏神经元的参数始终是一致的，并且运行在完全相同的功能下。</li>
<li>这个过程可以简单描绘为：如果神经元的权重被初始化为0， 在第一次前向传播的时候，除了输出之外，所有的中间层的节点的值都为零。而后第一次反向传播是，所有节点获取的梯度值都一样，更新后的网络参数将会相同。那么在下一次同样的前向传播-后向传播过程中，网络的参数会始终保持一致，无法打破这种对称性。</li>
</ul>
</li>
<li><p>因此对于权重矩阵要进行其它方式的初始化，如：随机初始化，让权重矩阵的参数分布遵从于一个标准正态分布。</p>
<ul>
<li>随机初始化时，一般喜欢使用较小的数值，这样在使用 <code>sigmoid</code> 或者 <code>tanh</code> 作为激活函数的时候，产生的梯度就不会太小。</li>
<li>随机初始化比较容易导致，梯度消失或者梯度爆炸问题，因此为了缓解这两个问题，我们可以根据不同的非线性激活函数用不同方法来初始化权重。</li>
<li>也就是<strong>初始化时，并不是服从标准正态分布，而是让 w 服从方差为 k/n 的正态分布</strong>，其中 k 因激活函数而不同。这些方法并不能完全解决梯度爆炸/消失的问题，但在很大程度上可以缓解。</li>
</ul>
</li>
<li><p>对于 tanh 激活函数，有对应的 <strong>Xavier初始化</strong>，实质是权重矩阵随机初始化后再乘以 np.sqrt(1/n)，其中n是当前层输入数据的维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以用如下代码表示</span></span><br><span class="line">w_l = np.random.randn(n_1,n_2) * np.sqrt(<span class="number">1</span>/n_1)</span><br><span class="line"><span class="comment"># w_l 表示第l层的权重矩阵</span></span><br><span class="line"><span class="comment"># 其中n_1是当前层输入数据的维度,n_2是当前层输出数据的维度</span></span><br></pre></td></tr></table></figure>
<ul>
<li>这样使得权重矩阵的方差在1的左右，这样权重矩阵不会过大过小，从而部分降低梯度消失/梯度爆炸的问题。</li>
</ul>
</li>
<li><p>对于 Relu 激活函数，有对应的  <strong>He Initialization</strong>，实质是权重矩阵随机初始化后再乘以 np.sqrt(2/n)，其中n是当前层输入数据的维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以用如下代码表示</span></span><br><span class="line">w_l = np.random.randn(n_1,n_2) * np.sqrt(<span class="number">2</span>/n_1)</span><br><span class="line"><span class="comment"># w_l 表示第l层的权重矩阵</span></span><br><span class="line"><span class="comment"># 其中n_1是当前层输入数据的维度,n_2是当前层输出数据的维度</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="前向传播与后向传播"><a href="#前向传播与后向传播" class="headerlink" title="前向传播与后向传播"></a>前向传播与后向传播</h2><ul>
<li>以一个有两个变量的逻辑回归为例，其中损失函数采用的是交叉熵函数，如下图所示：</li>
</ul>
<p><img src="/pictures/image-20200621170224254.png" alt="image-20200621170224254"></p>
<ul>
<li>用图示表示第 l 层的前向传播和后向传播：</li>
</ul>
<p><img src="/pictures/image-20200629095055434.png" alt="image-20200629095055434"></p>
<ul>
<li><p>整理一下，第i层的前向传播可表示为：</p>
<script type="math/tex; mode=display">
layer l :w^{[l]},b^{[l]} \\\\
input :a^{[l-1]} \\\\
output:a^{[l]} \\\\ 
z^{(i)} = w^{[l]^T}x^{(l)}+b \\\\
a^{(i)} = g^{[l]} (z^{(l)})</script></li>
<li><p>第 i 层的后向传播可表示为：</p>
<script type="math/tex; mode=display">
input :da^{[l]} \\\\
output:da^{[l-1]},dW^{[l]},db^{[l]} \\\\</script></li>
<li><p>详细计算可见下图：</p>
</li>
</ul>
<p><img src="/pictures/image-20200629103244101.png" alt="image-20200629103244101"></p>
<h2 id="零碎的细节"><a href="#零碎的细节" class="headerlink" title="零碎的细节"></a>零碎的细节</h2><ul>
<li><p>Loss Function 和 Cost Function 是有所不同的：</p>
<ul>
<li>Loss Function 针对单个样本；Cost Function 针对批次的样本或是整体数据Loss值的平均。</li>
</ul>
</li>
<li><p>Vectorization（向量化）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以用如下代码来解释什么是向量化</span></span><br><span class="line"><span class="comment"># 对于神经网络中的一层运算：Z=WT * X + b</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 非向量化的实现如下：</span></span><br><span class="line">z=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_x) :<span class="comment"># n_x x的维数</span></span><br><span class="line">    z+= w[i]*x[i]</span><br><span class="line">z+=b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向量化的实现</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">z = np.dot(w,x) <span class="comment"># wt * x</span></span><br><span class="line">z+=b</span><br></pre></td></tr></table></figure>
<ul>
<li>使用非向量化的实现，计算会非常耗费时间，没有很好的利用到CPU/GPU的并行计算能力，因此无论何时尽量<strong>避免使用 for 循环</strong>，而尝试使用 <strong>built-in 函数</strong>（一般都有向量化的实现），尽量的使用GPU/CPU的并行化运算能力。</li>
</ul>
</li>
<li><p>参数的维度问题</p>
<ul>
<li>当解决实际问题时，可以从逻辑回归开始尝试，将神经网络的深度当作超参数，逐步进行调试，找到比较合适的网络深度。</li>
</ul>
</li>
</ul>

    </div>

    
    
    
        <div class="reward-container">
  <div>觉着不错，打赏一下吧。</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/pictures/wechatpay.jpg" alt="刘涛 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/pictures/alipay.jpg" alt="刘涛 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>刘涛
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://lt4hyl.top/2020-08-21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89.html" title="深度学习笔记（1）">http://lt4hyl.top/2020-08-21/深度学习笔记（1）.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 基础知识，深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020-08-20/%E5%B0%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%80%9D%E8%B7%AF%E6%95%B4%E7%90%86.html" rel="prev" title="小目标检测思路整理">
      <i class="fa fa-chevron-left"></i> 小目标检测思路整理
    </a></div>
      <div class="post-nav-item">
    <a href="/2020-08-24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89.html" rel="next" title="深度学习笔记（2）">
      深度学习笔记（2） <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Neural-Networks-and-Deep-Learning"><span class="nav-number">1.</span> <span class="nav-text">Neural Networks and Deep Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E2%80%9CScale%E2%80%9D%E7%9A%84%E5%BC%95%E5%85%A5"><span class="nav-number">1.1.</span> <span class="nav-text">“Scale”的引入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Geffrey-Hinton-%E4%B8%93%E8%AE%BF"><span class="nav-number">1.2.</span> <span class="nav-text">Geffrey Hinton 专访</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E3%80%81%E5%88%86%E7%B1%BB%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="nav-number">1.3.</span> <span class="nav-text">回归、分类与逻辑回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pieter-Abbeel-%E4%B8%93%E8%AE%BF"><span class="nav-number">1.4.</span> <span class="nav-text">Pieter Abbeel 专访</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Activation-Functions"><span class="nav-number">1.5.</span> <span class="nav-text">激活函数(Activation Functions)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0%E9%97%AE%E9%A2%98"><span class="nav-number">1.6.</span> <span class="nav-text">初始化参数问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E5%90%8E%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">1.7.</span> <span class="nav-text">前向传播与后向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%B6%E7%A2%8E%E7%9A%84%E7%BB%86%E8%8A%82"><span class="nav-number">1.8.</span> <span class="nav-text">零碎的细节</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">刘涛</p>
  <div class="site-description" itemprop="description">为人民日益增长的美好生活需要而读书：机器学习、深度学习、c++、python、C#</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">30</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">京ICP备2020034167号-1 </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">刘涛</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">147k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">2:14</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;
      
      
      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          //console.log(error);
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"MxrUXwts7rlzq762NfLCiSVk-gzGzoHsz","app_key":"bqlnUht41VcOv8c5hK8DrOyz","server_url":"https://mxruxwts.lc-cn-n1-shared.com","security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@2.7.8/unpacked/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
